{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e71ead08-4e5a-49cf-8e87-333f6268d6fb",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-09-04T04:00:32.437990Z",
     "iopub.status.busy": "2024-09-04T04:00:32.437644Z",
     "iopub.status.idle": "2024-09-04T04:00:34.961740Z",
     "shell.execute_reply": "2024-09-04T04:00:34.961171Z",
     "shell.execute_reply.started": "2024-09-04T04:00:32.437967Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a5b60a-3dd5-4ec2-95f5-29570e64f508",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0a32750-5214-466a-b8a7-2f9e045a594a",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-09-04T04:00:44.286891Z",
     "iopub.status.busy": "2024-09-04T04:00:44.286461Z",
     "iopub.status.idle": "2024-09-04T04:00:45.233382Z",
     "shell.execute_reply": "2024-09-04T04:00:45.232830Z",
     "shell.execute_reply.started": "2024-09-04T04:00:44.286866Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import io\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_hdf5_file, transform=None, is_test=False):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.img_hdf5_file = img_hdf5_file\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        \n",
    "        # Process image IDs\n",
    "        self.image_ids = self.data['isic_id'].values\n",
    "        \n",
    "        # Process structured data\n",
    "        if 'target' in self.data.columns:\n",
    "            self.structured_data = self.data.drop(columns=['isic_id', 'target'])\n",
    "        else:\n",
    "            self.structured_data = self.data.drop(columns=['isic_id'])\n",
    "        \n",
    "        for col in self.structured_data.columns:\n",
    "            if self.structured_data[col].dtype == 'object':\n",
    "                self.structured_data[col] = self.structured_data[col].astype(str).str.extract('(\\d+)').astype(float)\n",
    "        \n",
    "        self.structured_data = self.structured_data.fillna(0).values.astype(np.float32)\n",
    "        \n",
    "        if not is_test:\n",
    "            self.targets = self.data['target'].values.astype(np.float32)\n",
    "        else:\n",
    "            self.targets = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.img_hdf5_file, 'r') as hdf5_file:\n",
    "            image_key = self.image_ids[idx]\n",
    "            image_data = hdf5_file[image_key][()]\n",
    "            \n",
    "        image = Image.open(io.BytesIO(image_data))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        structured_data = torch.tensor(self.structured_data[idx], dtype=torch.float32)\n",
    "        if self.targets is not None:\n",
    "            target = torch.tensor(self.targets[idx], dtype=torch.float32)\n",
    "            return image, structured_data, target\n",
    "        else:\n",
    "            return image, structured_data, self.image_ids[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e142e344-3631-4033-9504-cb0083b05d34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-04T04:00:47.774394Z",
     "iopub.status.busy": "2024-09-04T04:00:47.773948Z",
     "iopub.status.idle": "2024-09-04T04:00:47.778574Z",
     "shell.execute_reply": "2024-09-04T04:00:47.778036Z",
     "shell.execute_reply.started": "2024-09-04T04:00:47.774370Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_features(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_features = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, _, targets in dataloader:\n",
    "            images = images.to(device)\n",
    "            features = model(images)\n",
    "            all_features.append(features.cpu().numpy())\n",
    "            all_targets.append(targets.numpy())\n",
    "\n",
    "    features = np.vstack(all_features)\n",
    "    targets = np.hstack(all_targets)\n",
    "    return features, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be84103d-256c-4079-b8eb-a1301071779a",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-08-30T03:31:17.930477Z",
     "iopub.status.busy": "2024-08-30T03:31:17.930105Z",
     "iopub.status.idle": "2024-08-30T06:39:07.028237Z",
     "shell.execute_reply": "2024-08-30T06:39:07.027308Z",
     "shell.execute_reply.started": "2024-08-30T03:31:17.930458Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, AUC: 0.9166780029380396\n",
      "Epoch 2/50, AUC: 0.9570450566478176\n",
      "Epoch 3/50, AUC: 0.9209506369106312\n",
      "Epoch 4/50, AUC: 0.9432427102408791\n",
      "Epoch 5/50, AUC: 0.9348383072062464\n",
      "Epoch 6/50, AUC: 0.9398981747932305\n",
      "Epoch 7/50, AUC: 0.9397589197673717\n",
      "Epoch 8/50, AUC: 0.9446336506147748\n",
      "Epoch 9/50, AUC: 0.943432676634536\n",
      "Epoch 10/50, AUC: 0.9305632584066166\n",
      "Epoch 11/50, AUC: 0.9453428048216046\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 142\u001b[0m\n\u001b[1;32m    139\u001b[0m val_features \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    140\u001b[0m val_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, structured_data, targets \u001b[38;5;129;01min\u001b[39;00m val_loader:\n\u001b[1;32m    143\u001b[0m     features \u001b[38;5;241m=\u001b[39m combined_model(images, structured_data)\n\u001b[1;32m    144\u001b[0m     val_features\u001b[38;5;241m.\u001b[39mappend(features)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/utils/data/dataset.py:419\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/utils/data/dataset.py:419\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[0;32mIn[16], line 49\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m h5py\u001b[38;5;241m.\u001b[39mFile(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_hdf5_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m hdf5_file:\n\u001b[1;32m     48\u001b[0m     image_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_ids[idx]\n\u001b[0;32m---> 49\u001b[0m     image_data \u001b[38;5;241m=\u001b[39m \u001b[43mhdf5_file\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimage_key\u001b[49m\u001b[43m]\u001b[49m[()]\n\u001b[1;32m     51\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(io\u001b[38;5;241m.\u001b[39mBytesIO(image_data))\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/h5py/_hl/group.py:357\u001b[0m, in \u001b[0;36mGroup.__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid HDF5 object reference\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(name, (\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mstr\u001b[39m)):\n\u001b[0;32m--> 357\u001b[0m     oid \u001b[38;5;241m=\u001b[39m \u001b[43mh5o\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_e\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccessing a group is done with bytes or str, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    360\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(name)))\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5o.pyx:241\u001b[0m, in \u001b[0;36mh5py.h5o.open\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5i.pyx:43\u001b[0m, in \u001b[0;36mh5py.h5i.wrap_identifier\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:404\u001b[0m, in \u001b[0;36mparent\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "# 数据集类\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_hdf5_file, transform=None, is_test=False):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.img_hdf5_file = img_hdf5_file\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        \n",
    "        # 处理图像ID\n",
    "        self.image_ids = self.data['isic_id'].values\n",
    "        \n",
    "        # 处理结构化数据\n",
    "        if 'target' in self.data.columns:\n",
    "            self.structured_data = self.data.drop(columns=['isic_id', 'target'])\n",
    "        else:\n",
    "            self.structured_data = self.data.drop(columns=['isic_id'])\n",
    "        \n",
    "        for col in self.structured_data.columns:\n",
    "            if self.structured_data[col].dtype == 'object':\n",
    "                self.structured_data[col] = self.structured_data[col].astype(str).str.extract('(\\d+)').astype(float)\n",
    "        \n",
    "        self.structured_data = self.structured_data.fillna(0).values.astype(np.float32)\n",
    "        \n",
    "        if not is_test:\n",
    "            # 如果不是测试集，处理目标列\n",
    "            self.targets = self.data['target'].values.astype(np.float32)\n",
    "        else:\n",
    "            self.targets = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.img_hdf5_file, 'r') as hdf5_file:\n",
    "            image_key = self.image_ids[idx]\n",
    "            image_data = hdf5_file[image_key][()]\n",
    "            \n",
    "        image = Image.open(io.BytesIO(image_data))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        structured_data = torch.tensor(self.structured_data[idx], dtype=torch.float32)\n",
    "        if self.targets is not None:\n",
    "            target = torch.tensor(self.targets[idx], dtype=torch.float32)\n",
    "            return image, structured_data, target\n",
    "        else:\n",
    "            return image, structured_data, self.image_ids[idx]\n",
    "\n",
    "\n",
    "# 定义LeNet模型提取图像特征\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 6, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(6, 16, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_layers(x)\n",
    "\n",
    "# 定义结合LeNet特征和结构化特征的XGBoost模型\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, lenet_model, structured_data_dim):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.lenet = lenet_model\n",
    "        self.structured_data_dim = structured_data_dim\n",
    "\n",
    "    def forward(self, image, structured_data):\n",
    "        image_features = self.lenet(image).detach().cpu().numpy()\n",
    "        combined_features = np.hstack((image_features, structured_data.detach().cpu().numpy()))\n",
    "        return combined_features\n",
    "\n",
    "\n",
    "# 数据变换\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),  # LeNet接受32x32图像输入\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 加载数据集\n",
    "train_dataset = CustomDataset(\n",
    "    csv_file='data/train_metadata_aligned.csv',\n",
    "    img_hdf5_file='data/train-image.hdf5',\n",
    "    transform=transform\n",
    ")-\n",
    "\n",
    "# 划分训练集和验证集\n",
    "train_size = int(0.9 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_set, val_set = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=32, shuffle=False)\n",
    "\n",
    "# 初始化模型\n",
    "lenet_model = LeNet()\n",
    "combined_model = CombinedModel(lenet_model, structured_data_dim=train_set[0][1].shape[0])\n",
    "\n",
    "# XGBoost分类器\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', n_estimators=100)\n",
    "\n",
    "# 训练与验证\n",
    "best_auc = 0\n",
    "for epoch in range(50):\n",
    "    train_features = []\n",
    "    train_labels = []\n",
    "\n",
    "    # 训练过程\n",
    "    for images, structured_data, targets in train_loader:\n",
    "        features = combined_model(images, structured_data)\n",
    "        train_features.append(features)\n",
    "        train_labels.append(targets.numpy())\n",
    "    \n",
    "    train_features = np.vstack(train_features)\n",
    "    train_labels = np.hstack(train_labels)\n",
    "\n",
    "    xgb_model.fit(train_features, train_labels)\n",
    "\n",
    "    # 验证过程\n",
    "    val_features = []\n",
    "    val_labels = []\n",
    "\n",
    "    for images, structured_data, targets in val_loader:\n",
    "        features = combined_model(images, structured_data)\n",
    "        val_features.append(features)\n",
    "        val_labels.append(targets.numpy())\n",
    "\n",
    "    val_features = np.vstack(val_features)\n",
    "    val_labels = np.hstack(val_labels)\n",
    "    \n",
    "    val_preds = xgb_model.predict_proba(val_features)[:, 1]\n",
    "    auc = roc_auc_score(val_labels, val_preds)\n",
    "    print(f\"Epoch {epoch+1}/{50}, AUC: {auc}\")\n",
    "\n",
    "    # 保存最佳模型\n",
    "    if auc > best_auc:\n",
    "        best_auc = auc\n",
    "        xgb_model.save_model(f\"best_model_epoch_{epoch+1}.json\")\n",
    "\n",
    "# 测试过程\n",
    "test_dataset = CustomDataset(\n",
    "    csv_file='data/test_metadata_aligned.csv',\n",
    "    img_hdf5_file='data/test-image.hdf5',\n",
    "    transform=transform,\n",
    "    is_test=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "test_results = []\n",
    "for images, structured_data, image_ids in test_loader:\n",
    "    features = combined_model(images, structured_data)\n",
    "    test_preds = xgb_model.predict_proba(features)[:, 1]\n",
    "\n",
    "    for i in range(len(image_ids)):\n",
    "        test_results.append({'isic_id': image_ids[i], 'target': test_preds[i]})\n",
    "\n",
    "# 保存测试结果\n",
    "test_df = pd.DataFrame(test_results)\n",
    "test_df.to_csv('test_predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30a82960-f357-4cd4-821a-1876170aacf6",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-08-30T06:39:42.105918Z",
     "iopub.status.busy": "2024-08-30T06:39:42.105584Z",
     "iopub.status.idle": "2024-08-30T06:39:42.128441Z",
     "shell.execute_reply": "2024-08-30T06:39:42.127973Z",
     "shell.execute_reply.started": "2024-08-30T06:39:42.105900Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(\n",
    "    csv_file='data/test-metadata.csv',\n",
    "    img_hdf5_file='data/test-image.hdf5',\n",
    "    transform=transform,\n",
    "    is_test=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "test_results = []\n",
    "for images, structured_data, image_ids in test_loader:\n",
    "    features = combined_model(images, structured_data)\n",
    "    test_preds = xgb_model.predict_proba(features)[:, 1]\n",
    "\n",
    "    for i in range(len(image_ids)):\n",
    "        test_results.append({'isic_id': image_ids[i], 'target': test_preds[i]})\n",
    "\n",
    "# 保存测试结果\n",
    "test_df = pd.DataFrame(test_results)\n",
    "test_df.to_csv('test_predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b87ee0-c2e9-40f4-bcbe-cf71ac3d4c31",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.status.busy": "2024-08-30T08:33:33.533814Z",
     "iopub.status.idle": "2024-08-30T08:33:33.534023Z",
     "shell.execute_reply": "2024-08-30T08:33:33.533933Z",
     "shell.execute_reply.started": "2024-08-30T08:33:33.533924Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "\n",
    "# 假设你已经有了一个保存好的模型文件路径\n",
    "model_path = 'best_model_epoch_1.json'  # 将 'X' 替换为保存模型的具体轮数\n",
    "\n",
    "# 加载已经保存的XGBoost模型\n",
    "xgb_model = xgb.Booster()\n",
    "xgb_model.load_model(model_path)\n",
    "\n",
    "# 创建测试集\n",
    "test_dataset = CustomDataset(\n",
    "    csv_file='data/test-metadata.csv',\n",
    "    img_hdf5_file='data/test-image.hdf5',\n",
    "    transform=transform,\n",
    "    is_test=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# 进行测试并生成预测结果\n",
    "test_results = []\n",
    "for images, structured_data, image_ids in test_loader:\n",
    "    features = combined_model(images, structured_data)  # 提取图像和结构化特征\n",
    "    dtest = xgb.DMatrix(features)  # 创建DMatrix对象供XGBoost模型使用\n",
    "    test_preds = xgb_model.predict(dtest)  # 使用加载的模型进行预测\n",
    "\n",
    "    for i in range(len(image_ids)):\n",
    "        test_results.append({'isic_id': image_ids[i], 'target': test_preds[i]})\n",
    "\n",
    "# 保存测试结果到CSV文件\n",
    "test_df = pd.DataFrame(test_results)\n",
    "test_df.to_csv('test_predictions1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75093656-3635-40c2-b3eb-3df756f1dad2",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-08-30T08:34:59.409390Z",
     "iopub.status.busy": "2024-08-30T08:34:59.409152Z",
     "iopub.status.idle": "2024-08-30T21:44:40.560113Z",
     "shell.execute_reply": "2024-08-30T21:44:40.559351Z",
     "shell.execute_reply.started": "2024-08-30T08:34:59.409373Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3751/3106650346.py:17: DtypeWarning: Columns (51,52) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  self.data = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, AUC: 0.994080899132816\n",
      "Epoch 2/50, AUC: 0.9935276177164156\n",
      "Epoch 3/50, AUC: 0.9937861326639281\n",
      "Epoch 4/50, AUC: 0.9937504754297886\n",
      "Epoch 5/50, AUC: 0.9939923503347026\n",
      "Epoch 6/50, AUC: 0.9943013796972463\n",
      "Epoch 7/50, AUC: 0.99405296763274\n",
      "Epoch 8/50, AUC: 0.9936440980146052\n",
      "Epoch 9/50, AUC: 0.9934349089076526\n",
      "Epoch 10/50, AUC: 0.9940202818347786\n",
      "Epoch 11/50, AUC: 0.9938431842385517\n",
      "Epoch 12/50, AUC: 0.9940927848775293\n",
      "Epoch 13/50, AUC: 0.9946597349003499\n",
      "Epoch 14/50, AUC: 0.9941575621862163\n",
      "Epoch 15/50, AUC: 0.9933047600030427\n",
      "Epoch 16/50, AUC: 0.9950448330290582\n",
      "Epoch 17/50, AUC: 0.994568214666058\n",
      "Epoch 18/50, AUC: 0.9944844201658299\n",
      "Epoch 19/50, AUC: 0.9935638692377908\n",
      "Epoch 20/50, AUC: 0.9944891744637152\n",
      "Epoch 21/50, AUC: 0.9943245568994371\n",
      "Epoch 22/50, AUC: 0.9934889890460976\n",
      "Epoch 23/50, AUC: 0.9944499515061616\n",
      "Epoch 24/50, AUC: 0.9946086261980831\n",
      "Epoch 25/50, AUC: 0.9948172210178001\n",
      "Epoch 26/50, AUC: 0.9942740424844059\n",
      "Epoch 27/50, AUC: 0.9936084407804655\n",
      "Epoch 28/50, AUC: 0.9941094249201279\n",
      "Epoch 29/50, AUC: 0.9944802601551802\n",
      "Epoch 30/50, AUC: 0.993884784345048\n",
      "Epoch 31/50, AUC: 0.995189244827324\n",
      "Epoch 32/50, AUC: 0.9942716653354632\n",
      "Epoch 33/50, AUC: 0.9948582268370608\n",
      "Epoch 34/50, AUC: 0.9931645082154268\n",
      "Epoch 35/50, AUC: 0.9933493315457173\n",
      "Epoch 36/50, AUC: 0.9940702019625742\n",
      "Epoch 37/50, AUC: 0.9945307745702114\n",
      "Epoch 38/50, AUC: 0.9934711604290278\n",
      "Epoch 39/50, AUC: 0.9942799853567625\n",
      "Epoch 40/50, AUC: 0.9947714609006542\n",
      "Epoch 41/50, AUC: 0.9936880752700442\n",
      "Epoch 42/50, AUC: 0.9938633900045641\n",
      "Epoch 43/50, AUC: 0.9939168758557736\n",
      "Epoch 44/50, AUC: 0.994957472805416\n",
      "Epoch 45/50, AUC: 0.9935139491099955\n",
      "Epoch 46/50, AUC: 0.9947619523048836\n",
      "Epoch 47/50, AUC: 0.993565652099498\n",
      "Epoch 48/50, AUC: 0.9944178599954359\n",
      "Epoch 49/50, AUC: 0.9928768731933668\n",
      "Epoch 50/50, AUC: 0.9929576962574167\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/test_metadata_aligned.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 160\u001b[0m\n\u001b[1;32m    157\u001b[0m         xgb_model\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0830best_model_epoch_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# 测试过程\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mCustomDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcsv_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/test_metadata_aligned.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimg_hdf5_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/test-image.hdf5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    165\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    169\u001b[0m test_results \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[25], line 17\u001b[0m, in \u001b[0;36mCustomDataset.__init__\u001b[0;34m(self, csv_file, img_hdf5_file, transform, is_test)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, csv_file, img_hdf5_file, transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, is_test\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_hdf5_file \u001b[38;5;241m=\u001b[39m img_hdf5_file\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;241m=\u001b[39m transform\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/test_metadata_aligned.csv'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "# 数据集类\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_hdf5_file, transform=None, is_test=False):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.img_hdf5_file = img_hdf5_file\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        \n",
    "        # 处理图像ID\n",
    "        self.image_ids = self.data['isic_id'].values\n",
    "        \n",
    "        # 处理结构化数据\n",
    "        if 'target' in self.data.columns:\n",
    "            self.structured_data = self.data.drop(columns=['isic_id', 'target'])\n",
    "        else:\n",
    "            self.structured_data = self.data.drop(columns=['isic_id'])\n",
    "        \n",
    "        for col in self.structured_data.columns:\n",
    "            if self.structured_data[col].dtype == 'object':\n",
    "                self.structured_data[col] = self.structured_data[col].astype(str).str.extract('(\\d+)').astype(float)\n",
    "        \n",
    "        self.structured_data = self.structured_data.fillna(0).values.astype(np.float32)\n",
    "        \n",
    "        if not is_test:\n",
    "            # 如果不是测试集，处理目标列\n",
    "            self.targets = self.data['target'].values.astype(np.float32)\n",
    "        else:\n",
    "            self.targets = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.img_hdf5_file, 'r') as hdf5_file:\n",
    "            image_key = self.image_ids[idx]\n",
    "            image_data = hdf5_file[image_key][()]\n",
    "            \n",
    "        image = Image.open(io.BytesIO(image_data))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        structured_data = torch.tensor(self.structured_data[idx], dtype=torch.float32)\n",
    "        if self.targets is not None:\n",
    "            target = torch.tensor(self.targets[idx], dtype=torch.float32)\n",
    "            return image, structured_data, target\n",
    "        else:\n",
    "            return image, structured_data, self.image_ids[idx]\n",
    "\n",
    "\n",
    "# 定义LeNet模型提取图像特征\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 6, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(6, 16, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_layers(x)\n",
    "\n",
    "# 定义结合LeNet特征和结构化特征的XGBoost模型\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, lenet_model, structured_data_dim):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.lenet = lenet_model\n",
    "        self.structured_data_dim = structured_data_dim\n",
    "\n",
    "    def forward(self, image, structured_data):\n",
    "        image_features = self.lenet(image).detach().cpu().numpy()\n",
    "        combined_features = np.hstack((image_features, structured_data.detach().cpu().numpy()))\n",
    "        return combined_features\n",
    "\n",
    "\n",
    "# 数据变换\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),  # LeNet接受32x32图像输入\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 加载数据集\n",
    "train_dataset = CustomDataset(\n",
    "    csv_file='data/train-metadata.csv',\n",
    "    img_hdf5_file='data/train-image.hdf5',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# 划分训练集和验证集\n",
    "train_size = int(0.9 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_set, val_set = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=32, shuffle=False)\n",
    "\n",
    "# 初始化模型\n",
    "lenet_model = LeNet()\n",
    "combined_model = CombinedModel(lenet_model, structured_data_dim=train_set[0][1].shape[0])\n",
    "\n",
    "# XGBoost分类器\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', n_estimators=100)\n",
    "\n",
    "# 训练与验证\n",
    "best_auc = 0\n",
    "for epoch in range(50):\n",
    "    train_features = []\n",
    "    train_labels = []\n",
    "\n",
    "    # 训练过程\n",
    "    for images, structured_data, targets in train_loader:\n",
    "        features = combined_model(images, structured_data)\n",
    "        train_features.append(features)\n",
    "        train_labels.append(targets.numpy())\n",
    "    \n",
    "    train_features = np.vstack(train_features)\n",
    "    train_labels = np.hstack(train_labels)\n",
    "\n",
    "    xgb_model.fit(train_features, train_labels)\n",
    "\n",
    "    # 验证过程\n",
    "    val_features = []\n",
    "    val_labels = []\n",
    "\n",
    "    for images, structured_data, targets in val_loader:\n",
    "        features = combined_model(images, structured_data)\n",
    "        val_features.append(features)\n",
    "        val_labels.append(targets.numpy())\n",
    "\n",
    "    val_features = np.vstack(val_features)\n",
    "    val_labels = np.hstack(val_labels)\n",
    "    \n",
    "    val_preds = xgb_model.predict_proba(val_features)[:, 1]\n",
    "    auc = roc_auc_score(val_labels, val_preds)\n",
    "    print(f\"Epoch {epoch+1}/{50}, AUC: {auc}\")\n",
    "\n",
    "    # 保存最佳模型\n",
    "    if auc > best_auc:\n",
    "        best_auc = auc\n",
    "        xgb_model.save_model(f\"0830best_model_epoch_{epoch+1}.json\")\n",
    "\n",
    "# 测试过程\n",
    "test_dataset = CustomDataset(\n",
    "    csv_file='data/test_metadata_aligned.csv',\n",
    "    img_hdf5_file='data/test-image.hdf5',\n",
    "    transform=transform,\n",
    "    is_test=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "test_results = []\n",
    "for images, structured_data, image_ids in test_loader:\n",
    "    features = combined_model(images, structured_data)\n",
    "    test_preds = xgb_model.predict_proba(features)[:, 1]\n",
    "\n",
    "    for i in range(len(image_ids)):\n",
    "        test_results.append({'isic_id': image_ids[i], 'target': test_preds[i]})\n",
    "\n",
    "# 保存测试结果\n",
    "test_df = pd.DataFrame(test_results)\n",
    "test_df.to_csv('test_predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ce8d9f1-3470-4a24-912e-ed414019d62d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-04T04:05:33.342438Z",
     "iopub.status.busy": "2024-09-04T04:05:33.342012Z",
     "iopub.status.idle": "2024-09-04T09:11:23.405644Z",
     "shell.execute_reply": "2024-09-04T09:11:23.404394Z",
     "shell.execute_reply.started": "2024-09-04T04:05:33.342413Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520/1029356666.py:18: DtypeWarning: Columns (51,52) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  self.data = pd.read_csv(csv_file)\n",
      "/usr/local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=GoogLeNet_Weights.IMAGENET1K_V1`. You can also use `weights=GoogLeNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/googlenet-1378be20.pth\" to /root/.cache/torch/hub/checkpoints/googlenet-1378be20.pth\n",
      "100%|██████████| 49.7M/49.7M [00:09<00:00, 5.72MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, AUC: 0.9935246462802374\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 125\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# 训练过程\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, structured_data, targets \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m--> 125\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mcombined_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstructured_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m     train_features\u001b[38;5;241m.\u001b[39mappend(features)\n\u001b[1;32m    127\u001b[0m     train_labels\u001b[38;5;241m.\u001b[39mappend(targets\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 85\u001b[0m, in \u001b[0;36mCombinedModel.forward\u001b[0;34m(self, image, structured_data)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, image, structured_data):\n\u001b[0;32m---> 85\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgooglenet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     86\u001b[0m     combined_features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack((image_features, structured_data\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()))\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m combined_features\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 73\u001b[0m, in \u001b[0;36mGoogleNetFeatureExtractor.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 73\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten(x)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torchvision/models/googlenet.py:227\u001b[0m, in \u001b[0;36mInception.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 227\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(outputs, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torchvision/models/googlenet.py:221\u001b[0m, in \u001b[0;36mInception._forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    219\u001b[0m branch2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbranch2(x)\n\u001b[1;32m    220\u001b[0m branch3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbranch3(x)\n\u001b[0;32m--> 221\u001b[0m branch4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbranch4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m outputs \u001b[38;5;241m=\u001b[39m [branch1, branch2, branch3, branch4]\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/pooling.py:164\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_indices\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_jit_internal.py:497\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/functional.py:796\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    795\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[0;32m--> 796\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "from PIL import Image\n",
    "import io\n",
    "import torchvision.models as models\n",
    "\n",
    "# 数据集类\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_hdf5_file, transform=None, is_test=False):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.img_hdf5_file = img_hdf5_file\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        \n",
    "        # 处理图像ID\n",
    "        self.image_ids = self.data['isic_id'].values\n",
    "        \n",
    "        # 处理结构化数据\n",
    "        if 'target' in self.data.columns:\n",
    "            self.structured_data = self.data.drop(columns=['isic_id', 'target'])\n",
    "        else:\n",
    "            self.structured_data = self.data.drop(columns=['isic_id'])\n",
    "        \n",
    "        for col in self.structured_data.columns:\n",
    "            if self.structured_data[col].dtype == 'object':\n",
    "                self.structured_data[col] = self.structured_data[col].astype(str).str.extract('(\\d+)').astype(float)\n",
    "        \n",
    "        self.structured_data = self.structured_data.fillna(0).values.astype(np.float32)\n",
    "        \n",
    "        if not is_test:\n",
    "            # 如果不是测试集，处理目标列\n",
    "            self.targets = self.data['target'].values.astype(np.float32)\n",
    "        else:\n",
    "            self.targets = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.img_hdf5_file, 'r') as hdf5_file:\n",
    "            image_key = self.image_ids[idx]\n",
    "            image_data = hdf5_file[image_key][()]\n",
    "            \n",
    "        image = Image.open(io.BytesIO(image_data))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        structured_data = torch.tensor(self.structured_data[idx], dtype=torch.float32)\n",
    "        if self.targets is not None:\n",
    "            target = torch.tensor(self.targets[idx], dtype=torch.float32)\n",
    "            return image, structured_data, target\n",
    "        else:\n",
    "            return image, structured_data, self.image_ids[idx]\n",
    "\n",
    "# 定义GoogLeNet模型以提取图像特征\n",
    "class GoogleNetFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GoogleNetFeatureExtractor, self).__init__()\n",
    "        googlenet = models.googlenet(pretrained=True)\n",
    "        # 去掉最后的全连接层\n",
    "        self.features = nn.Sequential(*list(googlenet.children())[:-2])\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.flatten(x)\n",
    "        return x\n",
    "\n",
    "# 定义结合GoogLeNet特征和结构化特征的XGBoost模型\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, googlenet_model, structured_data_dim):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.googlenet = googlenet_model\n",
    "        self.structured_data_dim = structured_data_dim\n",
    "\n",
    "    def forward(self, image, structured_data):\n",
    "        image_features = self.googlenet(image).detach().cpu().numpy()\n",
    "        combined_features = np.hstack((image_features, structured_data.detach().cpu().numpy()))\n",
    "        return combined_features\n",
    "\n",
    "# 数据变换\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # GoogLeNet接受224x224图像输入\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 加载数据集\n",
    "train_dataset = CustomDataset(\n",
    "    csv_file='data/train-metadata.csv',\n",
    "    img_hdf5_file='data/train-image.hdf5',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# 划分训练集和验证集\n",
    "train_size = int(0.9 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_set, val_set = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=32, shuffle=False)\n",
    "\n",
    "# 初始化模型\n",
    "googlenet_model = GoogleNetFeatureExtractor()\n",
    "combined_model = CombinedModel(googlenet_model, structured_data_dim=train_set[0][1].shape[0])\n",
    "\n",
    "# XGBoost分类器\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', n_estimators=100)\n",
    "\n",
    "# 训练与验证\n",
    "best_auc = 0\n",
    "for epoch in range(50):\n",
    "    train_features = []\n",
    "    train_labels = []\n",
    "\n",
    "    # 训练过程\n",
    "    for images, structured_data, targets in train_loader:\n",
    "        features = combined_model(images, structured_data)\n",
    "        train_features.append(features)\n",
    "        train_labels.append(targets.numpy())\n",
    "    \n",
    "    train_features = np.vstack(train_features)\n",
    "    train_labels = np.hstack(train_labels)\n",
    "\n",
    "    xgb_model.fit(train_features, train_labels)\n",
    "\n",
    "    # 验证过程\n",
    "    val_features = []\n",
    "    val_labels = []\n",
    "\n",
    "    for images, structured_data, targets in val_loader:\n",
    "        features = combined_model(images, structured_data)\n",
    "        val_features.append(features)\n",
    "        val_labels.append(targets.numpy())\n",
    "\n",
    "    val_features = np.vstack(val_features)\n",
    "    val_labels = np.hstack(val_labels)\n",
    "    \n",
    "    val_preds = xgb_model.predict_proba(val_features)[:, 1]\n",
    "    auc = roc_auc_score(val_labels, val_preds)\n",
    "    print(f\"Epoch {epoch+1}/{50}, AUC: {auc}\")\n",
    "\n",
    "    # 保存最佳模型\n",
    "    if auc > best_auc:\n",
    "        best_auc = auc\n",
    "        xgb_model.save_model(f\"0904best_model_epoch_{epoch+1}.json\")\n",
    "\n",
    "# 测试过程\n",
    "test_dataset = CustomDataset(\n",
    "    csv_file='data/test_metadata_aligned.csv',\n",
    "    img_hdf5_file='data/test-image.hdf5',\n",
    "    transform=transform,\n",
    "    is_test=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "test_results = []\n",
    "for images, structured_data, image_ids in test_loader:\n",
    "    features = combined_model(images, structured_data)\n",
    "    test_preds = xgb_model.predict_proba(features)[:, 1]\n",
    "\n",
    "    for i in range(len(image_ids)):\n",
    "        test_results.append({'isic_id': image_ids[i], 'target': test_preds[i]})\n",
    "\n",
    "# 保存测试结果\n",
    "test_df = pd.DataFrame(test_results)\n",
    "test_df.to_csv('test_predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bae986a-e4d0-4fa7-875f-5abbbb287bf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23da0769-2a3f-4896-b119-69017a99dd37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-04T15:08:34.260259Z",
     "iopub.status.busy": "2024-09-04T15:08:34.259862Z",
     "iopub.status.idle": "2024-09-04T17:42:02.422720Z",
     "shell.execute_reply": "2024-09-04T17:42:02.421622Z",
     "shell.execute_reply.started": "2024-09-04T15:08:34.260229Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_413/2560157438.py:19: DtypeWarning: Columns (51,52) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  self.data = pd.read_csv(csv_file)\n",
      "/usr/local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:05<00:00, 7.83MB/s]\n",
      "/usr/local/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /torch/pytorch/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, AUC: 0.99106912984768\n",
      "Epoch 2/50, AUC: 0.9934282771919316\n",
      "Epoch 3/50, AUC: 0.9929475155393725\n",
      "Epoch 4/50, AUC: 0.9933960950594516\n",
      "Epoch 5/50, AUC: 0.994167152682545\n",
      "Epoch 6/50, AUC: 0.994653168560815\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 126\u001b[0m\n\u001b[1;32m    123\u001b[0m train_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# 训练过程\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, structured_data, targets \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m    127\u001b[0m     images, structured_data \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), structured_data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    128\u001b[0m     features \u001b[38;5;241m=\u001b[39m combined_model(images, structured_data)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/utils/data/dataset.py:419\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/utils/data/dataset.py:419\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[0;32mIn[1], line 51\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m h5py\u001b[38;5;241m.\u001b[39mFile(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_hdf5_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m hdf5_file:\n\u001b[1;32m     50\u001b[0m     image_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_ids[idx]\n\u001b[0;32m---> 51\u001b[0m     image_data \u001b[38;5;241m=\u001b[39m \u001b[43mhdf5_file\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimage_key\u001b[49m\u001b[43m]\u001b[49m[()]\n\u001b[1;32m     53\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(io\u001b[38;5;241m.\u001b[39mBytesIO(image_data))\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/h5py/_hl/group.py:357\u001b[0m, in \u001b[0;36mGroup.__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid HDF5 object reference\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(name, (\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mstr\u001b[39m)):\n\u001b[0;32m--> 357\u001b[0m     oid \u001b[38;5;241m=\u001b[39m \u001b[43mh5o\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_e\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccessing a group is done with bytes or str, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    360\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(name)))\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5o.pyx:241\u001b[0m, in \u001b[0;36mh5py.h5o.open\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5i.pyx:43\u001b[0m, in \u001b[0;36mh5py.h5i.wrap_identifier\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:404\u001b[0m, in \u001b[0;36mparent\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#resnet18\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "# 数据集类\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_hdf5_file, transform=None, is_test=False):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.img_hdf5_file = img_hdf5_file\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        \n",
    "        # 处理图像ID\n",
    "        self.image_ids = self.data['isic_id'].values\n",
    "        \n",
    "        # 处理结构化数据\n",
    "        if 'target' in self.data.columns:\n",
    "            self.structured_data = self.data.drop(columns=['isic_id', 'target'])\n",
    "        else:\n",
    "            self.structured_data = self.data.drop(columns=['isic_id'])\n",
    "        \n",
    "        for col in self.structured_data.columns:\n",
    "            if self.structured_data[col].dtype == 'object':\n",
    "                self.structured_data[col] = self.structured_data[col].astype(str).str.extract('(\\d+)').astype(float)\n",
    "        \n",
    "        self.structured_data = self.structured_data.fillna(0).values.astype(np.float32)\n",
    "        \n",
    "        if not is_test:\n",
    "            # 如果不是测试集，处理目标列\n",
    "            self.targets = self.data['target'].values.astype(np.float32)\n",
    "        else:\n",
    "            self.targets = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.img_hdf5_file, 'r') as hdf5_file:\n",
    "            image_key = self.image_ids[idx]\n",
    "            image_data = hdf5_file[image_key][()]\n",
    "            \n",
    "        image = Image.open(io.BytesIO(image_data))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        structured_data = torch.tensor(self.structured_data[idx], dtype=torch.float32)\n",
    "        if self.targets is not None:\n",
    "            target = torch.tensor(self.targets[idx], dtype=torch.float32)\n",
    "            return image, structured_data, target\n",
    "        else:\n",
    "            return image, structured_data, self.image_ids[idx]\n",
    "\n",
    "# 定义ResNet18模型提取图像特征\n",
    "class ResNet18FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet18FeatureExtractor, self).__init__()\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        self.features = nn.Sequential(*list(resnet.children())[:-1])  # 去掉最后的全连接层\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  # 展平成1D向量\n",
    "        return x\n",
    "\n",
    "# 定义结合ResNet18特征和结构化特征的XGBoost模型\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, resnet_model, structured_data_dim):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.resnet = resnet_model\n",
    "        self.structured_data_dim = structured_data_dim\n",
    "\n",
    "    def forward(self, image, structured_data):\n",
    "        image_features = self.resnet(image).detach().cpu().numpy()\n",
    "        combined_features = np.hstack((image_features, structured_data.detach().cpu().numpy()))\n",
    "        return combined_features\n",
    "\n",
    "# 判断是否有可用的GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 数据变换\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # ResNet18接受224x224图像输入\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 加载数据集\n",
    "train_dataset = CustomDataset(\n",
    "    csv_file='data/train-metadata.csv',\n",
    "    img_hdf5_file='data/train-image.hdf5',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# 划分训练集和验证集\n",
    "train_size = int(0.9 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_set, val_set = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=32, shuffle=False)\n",
    "\n",
    "# 初始化模型\n",
    "resnet_model = ResNet18FeatureExtractor().to(device)\n",
    "combined_model = CombinedModel(resnet_model, structured_data_dim=train_set[0][1].shape[0])\n",
    "\n",
    "# XGBoost分类器\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', n_estimators=100)\n",
    "\n",
    "# 训练与验证\n",
    "best_auc = 0\n",
    "for epoch in range(50):\n",
    "    train_features = []\n",
    "    train_labels = []\n",
    "\n",
    "    # 训练过程\n",
    "    for images, structured_data, targets in train_loader:\n",
    "        images, structured_data = images.to(device), structured_data.to(device)\n",
    "        features = combined_model(images, structured_data)\n",
    "        train_features.append(features)\n",
    "        train_labels.append(targets.numpy())\n",
    "    \n",
    "    train_features = np.vstack(train_features)\n",
    "    train_labels = np.hstack(train_labels)\n",
    "\n",
    "    xgb_model.fit(train_features, train_labels)\n",
    "\n",
    "    # 验证过程\n",
    "    val_features = []\n",
    "    val_labels = []\n",
    "\n",
    "    for images, structured_data, targets in val_loader:\n",
    "        images, structured_data = images.to(device), structured_data.to(device)\n",
    "        features = combined_model(images, structured_data)\n",
    "        val_features.append(features)\n",
    "        val_labels.append(targets.numpy())\n",
    "\n",
    "    val_features = np.vstack(val_features)\n",
    "    val_labels = np.hstack(val_labels)\n",
    "    \n",
    "    val_preds = xgb_model.predict_proba(val_features)[:, 1]\n",
    "    auc = roc_auc_score(val_labels, val_preds)\n",
    "    print(f\"Epoch {epoch+1}/{50}, AUC: {auc}\")\n",
    "\n",
    "    # 保存最佳模型\n",
    "    if auc > best_auc:\n",
    "        best_auc = auc\n",
    "        xgb_model.save_model(f\"resnet18_best_model_epoch_{epoch+1}.json\")\n",
    "\n",
    "# 测试过程\n",
    "test_dataset = CustomDataset(\n",
    "    csv_file='data/test_metadata_aligned.csv',\n",
    "    img_hdf5_file='data/test-image.hdf5',\n",
    "    transform=transform,\n",
    "    is_test=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "test_results = []\n",
    "for images, structured_data, image_ids in test_loader:\n",
    "    images, structured_data = images.to(device), structured_data.to(device)\n",
    "    features = combined_model(images, structured_data)\n",
    "    test_preds = xgb_model.predict_proba(features)[:, 1]\n",
    "\n",
    "    for i in range(len(image_ids)):\n",
    "        test_results.append({'isic_id': image_ids[i], 'target': test_preds[i]})\n",
    "\n",
    "# 保存测试结果\n",
    "test_df = pd.DataFrame(test_results)\n",
    "test_df.to_csv('test_predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec177ac-cba1-46d3-8319-006989d12521",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dda8b5cc-c182-41da-aaa2-09d978198ced",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-04T17:42:28.702692Z",
     "iopub.status.busy": "2024-09-04T17:42:28.702252Z",
     "iopub.status.idle": "2024-09-05T01:13:06.167354Z",
     "shell.execute_reply": "2024-09-05T01:13:06.166276Z",
     "shell.execute_reply.started": "2024-09-04T17:42:28.702667Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_413/3569154635.py:18: DtypeWarning: Columns (51,52) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  self.data = pd.read_csv(csv_file)\n",
      "/usr/local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n",
      "100%|██████████| 13.6M/13.6M [00:01<00:00, 8.85MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, AUC: 0.9943342069137652\n",
      "Epoch 2/50, AUC: 0.9945107493006791\n",
      "Epoch 3/50, AUC: 0.9929340431554821\n",
      "Epoch 4/50, AUC: 0.9944930950619877\n",
      "Epoch 5/50, AUC: 0.9944291745425878\n",
      "Epoch 6/50, AUC: 0.9952473571909062\n",
      "Epoch 7/50, AUC: 0.9936956104866177\n",
      "Epoch 8/50, AUC: 0.9949253195265012\n",
      "Epoch 9/50, AUC: 0.9949557578690725\n",
      "Epoch 10/50, AUC: 0.9961665951365617\n",
      "Epoch 11/50, AUC: 0.9952875358031006\n",
      "Epoch 12/50, AUC: 0.9951170810847008\n",
      "Epoch 13/50, AUC: 0.9936956104866178\n",
      "Epoch 14/50, AUC: 0.9957995087251509\n",
      "Epoch 15/50, AUC: 0.9941381839876056\n",
      "Epoch 16/50, AUC: 0.9939427698282973\n",
      "Epoch 17/50, AUC: 0.994662941013536\n",
      "Epoch 18/50, AUC: 0.9945728435195247\n",
      "Epoch 19/50, AUC: 0.9945831925559989\n",
      "Epoch 20/50, AUC: 0.996537334149081\n",
      "Epoch 21/50, AUC: 0.995480514895003\n",
      "Epoch 22/50, AUC: 0.9955736562232713\n",
      "Epoch 23/50, AUC: 0.9948108713584329\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 133\u001b[0m\n\u001b[1;32m    130\u001b[0m train_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# 训练过程\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, structured_data, targets \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m    134\u001b[0m     images, structured_data \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), structured_data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    135\u001b[0m     features \u001b[38;5;241m=\u001b[39m combined_model(images, structured_data)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/utils/data/dataset.py:419\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/utils/data/dataset.py:419\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[0;32mIn[2], line 54\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mh5py\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimg_hdf5_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m hdf5_file:\n\u001b[1;32m     55\u001b[0m         image_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_ids[idx]\n\u001b[1;32m     56\u001b[0m         image_data \u001b[38;5;241m=\u001b[39m hdf5_file[image_key][()]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/h5py/_hl/files.py:553\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    546\u001b[0m     warn(\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mswmr=True only affects read (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m) mode. For swmr write \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    548\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode, set f.swmr_mode = True after opening the file.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    549\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    550\u001b[0m     )\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m phil:\n\u001b[0;32m--> 553\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fapl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlibver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrdcc_nslots\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrdcc_nbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrdcc_w0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mlocking\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpage_buf_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_meta_keep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_raw_keep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m                     \u001b[49m\u001b[43malignment_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malignment_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m                     \u001b[49m\u001b[43malignment_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malignment_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmeta_block_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeta_block_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[1;32m    560\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[1;32m    561\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[1;32m    562\u001b[0m     fid \u001b[38;5;241m=\u001b[39m make_fid(name, mode, userblock_size, fapl, fcpl, swmr\u001b[38;5;241m=\u001b[39mswmr)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/h5py/_hl/files.py:127\u001b[0m, in \u001b[0;36mmake_fapl\u001b[0;34m(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0, locking, page_buf_size, min_meta_keep, min_raw_keep, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;66;03m# we default to earliest\u001b[39;00m\n\u001b[1;32m    126\u001b[0m     low, high \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mLIBVER_EARLIEST, h5f\u001b[38;5;241m.\u001b[39mLIBVER_LATEST\n\u001b[0;32m--> 127\u001b[0m \u001b[43mplist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_libver_bounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhigh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m plist\u001b[38;5;241m.\u001b[39mset_alignment(alignment_threshold, alignment_interval)\n\u001b[1;32m    130\u001b[0m cache_settings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(plist\u001b[38;5;241m.\u001b[39mget_cache())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "from PIL import Image\n",
    "import io\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 数据集类\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_hdf5_file, transform=None, is_test=False):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.img_hdf5_file = img_hdf5_file\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        \n",
    "        # 处理图像ID\n",
    "        self.image_ids = self.data['isic_id'].values\n",
    "        \n",
    "        # 处理结构化数据\n",
    "        if 'target' in self.data.columns:\n",
    "            self.structured_data = self.data.drop(columns=['isic_id', 'target'])\n",
    "        else:\n",
    "            self.structured_data = self.data.drop(columns=['isic_id'])\n",
    "        \n",
    "        # 特征工程：将字符串类型的列转换为数字\n",
    "        for col in self.structured_data.columns:\n",
    "            if self.structured_data[col].dtype == 'object':\n",
    "                self.structured_data[col] = self.structured_data[col].astype(str).str.extract('(\\d+)').astype(float)\n",
    "        \n",
    "        # 处理缺失值\n",
    "        self.structured_data = self.structured_data.fillna(0).values.astype(np.float32)\n",
    "        \n",
    "        # 特征标准化\n",
    "        self.scaler = StandardScaler()\n",
    "        self.structured_data = self.scaler.fit_transform(self.structured_data)\n",
    "        \n",
    "        if not is_test:\n",
    "            # 如果不是测试集，处理目标列\n",
    "            self.targets = self.data['target'].values.astype(np.float32)\n",
    "        else:\n",
    "            self.targets = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.img_hdf5_file, 'r') as hdf5_file:\n",
    "            image_key = self.image_ids[idx]\n",
    "            image_data = hdf5_file[image_key][()]\n",
    "            \n",
    "        image = Image.open(io.BytesIO(image_data))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        structured_data = torch.tensor(self.structured_data[idx], dtype=torch.float32)\n",
    "        if self.targets is not None:\n",
    "            target = torch.tensor(self.targets[idx], dtype=torch.float32)\n",
    "            return image, structured_data, target\n",
    "        else:\n",
    "            return image, structured_data, self.image_ids[idx]\n",
    "\n",
    "# 定义MobileNetV1模型提取图像特征\n",
    "class MobileNetV1FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MobileNetV1FeatureExtractor, self).__init__()\n",
    "        mobilenet = models.mobilenet_v2(pretrained=True)\n",
    "        self.features = mobilenet.features\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)  # 自适应平均池化\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "# 定义结合MobileNetV1特征和结构化特征的XGBoost模型\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, mobilenet_model, structured_data_dim):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.mobilenet = mobilenet_model\n",
    "        self.structured_data_dim = structured_data_dim\n",
    "\n",
    "    def forward(self, image, structured_data):\n",
    "        image_features = self.mobilenet(image).detach().cpu().numpy()\n",
    "        combined_features = np.hstack((image_features, structured_data.detach().cpu().numpy()))\n",
    "        return combined_features\n",
    "\n",
    "# 判断是否有可用的GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 数据变换\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((15, 15)),  # MobileNetV1接受15x15图像输入\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 加载数据集\n",
    "train_dataset = CustomDataset(\n",
    "    csv_file='data/train-metadata.csv',\n",
    "    img_hdf5_file='data/train-image.hdf5',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# 划分训练集和验证集\n",
    "train_size = int(0.9 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_set, val_set = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=32, shuffle=False)\n",
    "\n",
    "# 初始化模型\n",
    "mobilenet_model = MobileNetV1FeatureExtractor().to(device)\n",
    "combined_model = CombinedModel(mobilenet_model, structured_data_dim=train_set[0][1].shape[0])\n",
    "\n",
    "# XGBoost分类器\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', n_estimators=100)\n",
    "\n",
    "# 训练与验证\n",
    "best_auc = 0\n",
    "for epoch in range(50):\n",
    "    train_features = []\n",
    "    train_labels = []\n",
    "\n",
    "    # 训练过程\n",
    "    for images, structured_data, targets in train_loader:\n",
    "        images, structured_data = images.to(device), structured_data.to(device)\n",
    "        features = combined_model(images, structured_data)\n",
    "        train_features.append(features)\n",
    "        train_labels.append(targets.numpy())\n",
    "    \n",
    "    train_features = np.vstack(train_features)\n",
    "    train_labels = np.hstack(train_labels)\n",
    "\n",
    "    xgb_model.fit(train_features, train_labels)\n",
    "\n",
    "    # 验证过程\n",
    "    val_features = []\n",
    "    val_labels = []\n",
    "\n",
    "    for images, structured_data, targets in val_loader:\n",
    "        images, structured_data = images.to(device), structured_data.to(device)\n",
    "        features = combined_model(images, structured_data)\n",
    "        val_features.append(features)\n",
    "        val_labels.append(targets.numpy())\n",
    "\n",
    "    val_features = np.vstack(val_features)\n",
    "    val_labels = np.hstack(val_labels)\n",
    "    \n",
    "    val_preds = xgb_model.predict_proba(val_features)[:, 1]\n",
    "    auc = roc_auc_score(val_labels, val_preds)\n",
    "    print(f\"Epoch {epoch+1}/{50}, AUC: {auc}\")\n",
    "\n",
    "    # 保存最佳模型\n",
    "    if auc > best_auc:\n",
    "        best_auc = auc\n",
    "        xgb_model.save_model(f\"mobilenet_best_model_epoch_{epoch+1}.json\")\n",
    "\n",
    "# 测试过程\n",
    "test_dataset = CustomDataset(\n",
    "    csv_file='data/test_metadata_aligned.csv',\n",
    "    img_hdf5_file='data/test-image.hdf5',\n",
    "    transform=transform,\n",
    "    is_test=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "test_results = []\n",
    "for images, structured_data, image_ids in test_loader:\n",
    "    images, structured_data = images.to(device), structured_data.to(device)\n",
    "    features = combined_model(images, structured_data)\n",
    "    test_preds = xgb_model.predict_proba(features)[:, 1]\n",
    "\n",
    "    for i in range(len(image_ids)):\n",
    "        test_results.append({'isic_id': image_ids[i], 'target': test_preds[i]})\n",
    "\n",
    "# 保存测试结果\n",
    "test_df = pd.DataFrame(test_results)\n",
    "test_df.to_csv('test_predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6822832-e861-4c23-9eda-8be43178ea46",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-09-05T01:28:46.607346Z",
     "iopub.status.busy": "2024-09-05T01:28:46.606835Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# 定义MobileNetV1模型提取图像特征\n",
    "class MobileNetV1FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MobileNetV1FeatureExtractor, self).__init__()\n",
    "        mobilenet = models.mobilenet_v2(pretrained=True)\n",
    "        self.features = mobilenet.features\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)  # 自适应平均池化\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "# 定义结合MobileNetV1特征和结构化特征的SVM和XGBoost模型\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, mobilenet_model, structured_data_dim):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.mobilenet = mobilenet_model\n",
    "        self.structured_data_dim = structured_data_dim\n",
    "        self.svm = SVC(probability=True)  # 定义SVM分类器\n",
    "\n",
    "    def forward(self, image, structured_data):\n",
    "        # 提取图像特征\n",
    "        with torch.no_grad():\n",
    "            image_features = self.mobilenet(image).detach().cpu().numpy()\n",
    "        return image_features, structured_data.detach().cpu().numpy()\n",
    "\n",
    "# 初始化模型\n",
    "mobilenet_model = MobileNetV1FeatureExtractor().to(device)\n",
    "combined_model = CombinedModel(mobilenet_model, structured_data_dim=train_set[0][1].shape[0])\n",
    "\n",
    "# XGBoost分类器\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', n_estimators=100)\n",
    "\n",
    "# SVM模型初始化\n",
    "svm_model = SVC(probability=True)\n",
    "\n",
    "# 训练与验证\n",
    "best_auc = 0\n",
    "for epoch in range(50):\n",
    "    train_image_features = []\n",
    "    train_structured_features = []\n",
    "    train_labels = []\n",
    "\n",
    "    # 训练过程\n",
    "    for images, structured_data, targets in train_loader:\n",
    "        images, structured_data = images.to(device), structured_data.to(device)\n",
    "        image_features, structured_features = combined_model(images, structured_data)\n",
    "        \n",
    "        train_image_features.append(image_features)\n",
    "        train_structured_features.append(structured_features)\n",
    "        train_labels.append(targets.numpy())\n",
    "\n",
    "    train_image_features = np.vstack(train_image_features)\n",
    "    train_structured_features = np.vstack(train_structured_features)\n",
    "    train_labels = np.hstack(train_labels)\n",
    "\n",
    "    # 先用SVM拟合图像特征\n",
    "    svm_model.fit(train_image_features, train_labels)\n",
    "\n",
    "    # 获取SVM预测概率作为新的图像特征\n",
    "    svm_train_probs = svm_model.predict_proba(train_image_features)[:, 1]\n",
    "\n",
    "    # 将SVM预测的概率与结构化特征组合\n",
    "    combined_train_features = np.hstack((svm_train_probs.reshape(-1, 1), train_structured_features))\n",
    "\n",
    "    # 用XGBoost拟合组合特征\n",
    "    xgb_model.fit(combined_train_features, train_labels)\n",
    "\n",
    "    # 验证过程\n",
    "    val_image_features = []\n",
    "    val_structured_features = []\n",
    "    val_labels = []\n",
    "\n",
    "    for images, structured_data, targets in val_loader:\n",
    "        images, structured_data = images.to(device), structured_data.to(device)\n",
    "        image_features, structured_features = combined_model(images, structured_data)\n",
    "\n",
    "        val_image_features.append(image_features)\n",
    "        val_structured_features.append(structured_features)\n",
    "        val_labels.append(targets.numpy())\n",
    "\n",
    "    val_image_features = np.vstack(val_image_features)\n",
    "    val_structured_features = np.vstack(val_structured_features)\n",
    "    val_labels = np.hstack(val_labels)\n",
    "\n",
    "    # 使用SVM预测验证集图像特征\n",
    "    svm_val_probs = svm_model.predict_proba(val_image_features)[:, 1]\n",
    "\n",
    "    # 将SVM预测的概率与结构化特征组合\n",
    "    combined_val_features = np.hstack((svm_val_probs.reshape(-1, 1), val_structured_features))\n",
    "\n",
    "    # XGBoost在验证集上预测\n",
    "    val_preds = xgb_model.predict_proba(combined_val_features)[:, 1]\n",
    "    auc = roc_auc_score(val_labels, val_preds)\n",
    "    print(f\"Epoch {epoch+1}/{50}, AUC: {auc}\")\n",
    "\n",
    "    # 保存最佳模型\n",
    "    if auc > best_auc:\n",
    "        best_auc = auc\n",
    "        xgb_model.save_model(f\"svm_xgb_best_model_epoch_{epoch+1}.json\")\n",
    "\n",
    "# 测试过程\n",
    "test_dataset = CustomDataset(\n",
    "    csv_file='data/test_metadata_aligned.csv',\n",
    "    img_hdf5_file='data/test-image.hdf5',\n",
    "    transform=transform,\n",
    "    is_test=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "test_results = []\n",
    "for images, structured_data, image_ids in test_loader:\n",
    "    images, structured_data = images.to(device), structured_data.to(device)\n",
    "    image_features, structured_features = combined_model(images, structured_data)\n",
    "    \n",
    "    # SVM预测测试集图像特征\n",
    "    svm_test_probs = svm_model.predict_proba(image_features)[:, 1]\n",
    "\n",
    "    # 将SVM预测的概率与结构化特征组合\n",
    "    combined_test_features = np.hstack((svm_test_probs.reshape(-1, 1), structured_features))\n",
    "\n",
    "    # XGBoost在测试集上预测\n",
    "    test_preds = xgb_model.predict_proba(combined_test_features)[:, 1]\n",
    "\n",
    "    for i in range(len(image_ids)):\n",
    "        test_results.append({'isic_id': image_ids[i], 'target': test_preds[i]})\n",
    "\n",
    "# 保存测试结果\n",
    "test_df = pd.DataFrame(test_results)\n",
    "test_df.to_csv('test_predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cae4be3-d3b5-4dbd-8624-7381bb40d3ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87317ade-6f06-419b-b898-b8547b5cd6b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "from PIL import Image\n",
    "import io\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 数据集类\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_hdf5_file, transform=None, is_test=False):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.img_hdf5_file = img_hdf5_file\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        \n",
    "        # 处理图像ID\n",
    "        self.image_ids = self.data['isic_id'].values\n",
    "        \n",
    "        # 处理结构化数据\n",
    "        if 'target' in self.data.columns:\n",
    "            self.structured_data = self.data.drop(columns=['isic_id', 'target'])\n",
    "        else:\n",
    "            self.structured_data = self.data.drop(columns=['isic_id'])\n",
    "        \n",
    "        # 特征工程：将字符串类型的列转换为数字\n",
    "        for col in self.structured_data.columns:\n",
    "            if self.structured_data[col].dtype == 'object':\n",
    "                self.structured_data[col] = self.structured_data[col].astype(str).str.extract('(\\d+)').astype(float)\n",
    "        \n",
    "        # 处理缺失值\n",
    "        self.structured_data = self.structured_data.fillna(0).values.astype(np.float32)\n",
    "        \n",
    "        # 特征标准化\n",
    "        self.scaler = StandardScaler()\n",
    "        self.structured_data = self.scaler.fit_transform(self.structured_data)\n",
    "        \n",
    "        if not is_test:\n",
    "            # 如果不是测试集，处理目标列\n",
    "            self.targets = self.data['target'].values.astype(np.float32)\n",
    "        else:\n",
    "            self.targets = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.img_hdf5_file, 'r') as hdf5_file:\n",
    "            image_key = self.image_ids[idx]\n",
    "            image_data = hdf5_file[image_key][()]\n",
    "            \n",
    "        image = Image.open(io.BytesIO(image_data))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        structured_data = torch.tensor(self.structured_data[idx], dtype=torch.float32)\n",
    "        if self.targets is not None:\n",
    "            target = torch.tensor(self.targets[idx], dtype=torch.float32)\n",
    "            return image, structured_data, target\n",
    "        else:\n",
    "            return image, structured_data, self.image_ids[idx]\n",
    "\n",
    "# 定义MobileNetV1模型提取图像特征\n",
    "class MobileNetV1FeatureExtractor(nn.Module):\n",
    "    def __init__(self, weights_path=None):\n",
    "        super(MobileNetV1FeatureExtractor, self).__init__()\n",
    "        mobilenet = models.mobilenet_v2(pretrained=False)\n",
    "        self.features = mobilenet.features\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)  # 自适应平均池化\n",
    "        \n",
    "        # 加载本地权重\n",
    "        if weights_path:\n",
    "            self.load_state_dict(torch.load(weights_path))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "# 定义结合MobileNetV1特征和结构化特征的XGBoost模型\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, mobilenet_model, structured_data_dim):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.mobilenet = mobilenet_model\n",
    "        self.structured_data_dim = structured_data_dim\n",
    "\n",
    "    def forward(self, image, structured_data):\n",
    "        image_features = self.mobilenet(image).detach().cpu().numpy()\n",
    "        combined_features = np.hstack((image_features, structured_data.detach().cpu().numpy()))\n",
    "        return combined_features\n",
    "\n",
    "# 判断是否有可用的GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 数据变换\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((15, 15)),  # MobileNetV1接受15x15图像输入\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 加载数据集\n",
    "train_dataset = CustomDataset(\n",
    "    csv_file='data/train-metadata.csv',\n",
    "    img_hdf5_file='data/train-image.hdf5',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# 划分训练集和验证集\n",
    "train_size = int(0.9 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_set, val_set = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=32, shuffle=False)\n",
    "\n",
    "# 初始化模型\n",
    "mobilenet_weights_path = 'data/mobilenet_v2-b0353104.pth'\n",
    "mobilenet_model = MobileNetV1FeatureExtractor(weights_path=mobilenet_weights_path).to(device)\n",
    "combined_model = CombinedModel(mobilenet_model, structured_data_dim=train_set[0][1].shape[0])\n",
    "\n",
    "# XGBoost分类器\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', n_estimators=100)\n",
    "\n",
    "# 训练与验证\n",
    "best_auc = 0\n",
    "for epoch in range(50):\n",
    "    train_features = []\n",
    "    train_labels = []\n",
    "\n",
    "    # 训练过程\n",
    "    for images, structured_data, targets in train_loader:\n",
    "        images, structured_data = images.to(device), structured_data.to(device)\n",
    "        features = combined_model(images, structured_data)\n",
    "        train_features.append(features)\n",
    "        train_labels.append(targets.numpy())\n",
    "    \n",
    "    train_features = np.vstack(train_features)\n",
    "    train_labels = np.hstack(train_labels)\n",
    "\n",
    "    xgb_model.fit(train_features, train_labels)\n",
    "\n",
    "    # 验证过程\n",
    "    val_features = []\n",
    "    val_labels = []\n",
    "\n",
    "    for images, structured_data, targets in val_loader:\n",
    "        images, structured_data = images.to(device), structured_data.to(device)\n",
    "        features = combined_model(images, structured_data)\n",
    "        val_features.append(features)\n",
    "        val_labels.append(targets.numpy())\n",
    "\n",
    "    val_features = np.vstack(val_features)\n",
    "    val_labels = np.hstack(val_labels)\n",
    "    \n",
    "    val_preds = xgb_model.predict_proba(val_features)[:, 1]\n",
    "    auc = roc_auc_score(val_labels, val_preds)\n",
    "    print(f\"Epoch {epoch+1}/{50}, AUC: {auc}\")\n",
    "\n",
    "    # 保存最佳模型\n",
    "    if auc > best_auc:\n",
    "        best_auc = auc\n",
    "        xgb_model.save_model(f\"mobilenet_best_model_epoch_{epoch+1}.json\")\n",
    "\n",
    "# 测试过程\n",
    "test_dataset = CustomDataset(\n",
    "    csv_file='data/test-metadata-aligned.csv',\n",
    "    img_hdf5_file='data/test-image.hdf5',\n",
    "    transform=transform,\n",
    "    is_test=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "test_results = []\n",
    "for images, structured_data, image_ids in test_loader:\n",
    "    images, structured_data = images.to(device), structured_data.to(device)\n",
    "    features = combined_model(images, structured_data)\n",
    "    test_preds = xgb_model.predict_proba(features)[:, 1]\n",
    "\n",
    "    for i in range(len(image_ids)):\n",
    "        test_results.append({'isic_id': image_ids[i], 'target': test_preds[i]})\n",
    "\n",
    "# 保存测试结果\n",
    "test_df = pd.DataFrame(test_results)\n",
    "test_df.to_csv('test_predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd43d73d-eba0-438c-a4a3-1c8660f16dbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fd9587e-2570-4593-8848-8b5e25f1d11e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T17:20:28.864368Z",
     "iopub.status.busy": "2024-09-05T17:20:28.863775Z",
     "iopub.status.idle": "2024-09-05T17:20:48.879692Z",
     "shell.execute_reply": "2024-09-05T17:20:48.878969Z",
     "shell.execute_reply.started": "2024-09-05T17:20:28.864343Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_282/1327625922.py:18: DtypeWarning: Columns (51,52) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  self.data = pd.read_csv(csv_file)\n",
      "/usr/local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for MobileNetV1FeatureExtractor:\n\tUnexpected key(s) in state_dict: \"classifier.1.weight\", \"classifier.1.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 125\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# 初始化模型\u001b[39;00m\n\u001b[1;32m    124\u001b[0m mobilenet_weights_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/mobilenet_v2-b0353104.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 125\u001b[0m mobilenet_model \u001b[38;5;241m=\u001b[39m \u001b[43mMobileNetV1FeatureExtractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmobilenet_weights_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    126\u001b[0m combined_model \u001b[38;5;241m=\u001b[39m CombinedModel(mobilenet_model, structured_data_dim\u001b[38;5;241m=\u001b[39mtrain_set[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# XGBoost分类器\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 79\u001b[0m, in \u001b[0;36mMobileNetV1FeatureExtractor.__init__\u001b[0;34m(self, weights_path)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# 加载本地权重\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_path:\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:2189\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2184\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2185\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2186\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2190\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for MobileNetV1FeatureExtractor:\n\tUnexpected key(s) in state_dict: \"classifier.1.weight\", \"classifier.1.bias\". "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "from PIL import Image\n",
    "import io\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 数据集类\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_hdf5_file, transform=None, is_test=False):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.img_hdf5_file = img_hdf5_file\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        \n",
    "        # 处理图像ID\n",
    "        self.image_ids = self.data['isic_id'].values\n",
    "        \n",
    "        # 处理结构化数据\n",
    "        if 'target' in self.data.columns:\n",
    "            self.structured_data = self.data.drop(columns=['isic_id', 'target'])\n",
    "        else:\n",
    "            self.structured_data = self.data.drop(columns=['isic_id'])\n",
    "        \n",
    "        # 特征工程：将字符串类型的列转换为数字\n",
    "        for col in self.structured_data.columns:\n",
    "            if self.structured_data[col].dtype == 'object':\n",
    "                self.structured_data[col] = self.structured_data[col].astype(str).str.extract('(\\d+)').astype(float)\n",
    "        \n",
    "        # 处理缺失值\n",
    "        self.structured_data = self.structured_data.fillna(0).values.astype(np.float32)\n",
    "        \n",
    "        # 特征标准化\n",
    "        self.scaler = StandardScaler()\n",
    "        self.structured_data = self.scaler.fit_transform(self.structured_data)\n",
    "        \n",
    "        if not is_test:\n",
    "            # 如果不是测试集，处理目标列\n",
    "            self.targets = self.data['target'].values.astype(np.float32)\n",
    "        else:\n",
    "            self.targets = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.img_hdf5_file, 'r') as hdf5_file:\n",
    "            image_key = self.image_ids[idx]\n",
    "            image_data = hdf5_file[image_key][()]\n",
    "            \n",
    "        image = Image.open(io.BytesIO(image_data))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        structured_data = torch.tensor(self.structured_data[idx], dtype=torch.float32)\n",
    "        if self.targets is not None:\n",
    "            target = torch.tensor(self.targets[idx], dtype=torch.float32)\n",
    "            return image, structured_data, target\n",
    "        else:\n",
    "            return image, structured_data, self.image_ids[idx]\n",
    "\n",
    "# 定义MobileNetV1模型提取图像特征\n",
    "class MobileNetV1FeatureExtractor(nn.Module):\n",
    "    def __init__(self, weights_path=None):\n",
    "        super(MobileNetV1FeatureExtractor, self).__init__()\n",
    "        mobilenet = models.mobilenet_v2(pretrained=False)\n",
    "        self.features = mobilenet.features\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)  # 自适应平均池化\n",
    "        \n",
    "        # 加载本地权重\n",
    "        if weights_path:\n",
    "            self.load_state_dict(torch.load(weights_path))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "# 定义结合MobileNetV1特征和结构化特征的XGBoost模型\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, mobilenet_model, structured_data_dim):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.mobilenet = mobilenet_model\n",
    "        self.structured_data_dim = structured_data_dim\n",
    "\n",
    "    def forward(self, image, structured_data):\n",
    "        image_features = self.mobilenet(image).detach().cpu().numpy()\n",
    "        combined_features = np.hstack((image_features, structured_data.detach().cpu().numpy()))\n",
    "        return combined_features\n",
    "\n",
    "# 判断是否有可用的GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 数据变换\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((15, 15)),  # MobileNetV1接受15x15图像输入\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 加载数据集\n",
    "train_dataset = CustomDataset(\n",
    "    csv_file='data/train-metadata.csv',\n",
    "    img_hdf5_file='data/train-image.hdf5',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# 划分训练集和验证集\n",
    "train_size = int(0.9 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_set, val_set = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=32, shuffle=False)\n",
    "\n",
    "# 初始化模型\n",
    "mobilenet_weights_path = 'data/mobilenet_v2-b0353104.pth'\n",
    "mobilenet_model = MobileNetV1FeatureExtractor(weights_path=mobilenet_weights_path).to(device)\n",
    "combined_model = CombinedModel(mobilenet_model, structured_data_dim=train_set[0][1].shape[0])\n",
    "\n",
    "# XGBoost分类器\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', n_estimators=100)\n",
    "\n",
    "# 训练与验证\n",
    "best_auc = 0\n",
    "for epoch in range(1):\n",
    "    train_features = []\n",
    "    train_labels = []\n",
    "\n",
    "    # 训练过程\n",
    "    for images, structured_data, targets in train_loader:\n",
    "        images, structured_data = images.to(device), structured_data.to(device)\n",
    "        features = combined_model(images, structured_data)\n",
    "        train_features.append(features)\n",
    "        train_labels.append(targets.numpy())\n",
    "    \n",
    "    train_features = np.vstack(train_features)\n",
    "    train_labels = np.hstack(train_labels)\n",
    "\n",
    "    xgb_model.fit(train_features, train_labels)\n",
    "\n",
    "    # 验证过程\n",
    "    val_features = []\n",
    "    val_labels = []\n",
    "\n",
    "    for images, structured_data, targets in val_loader:\n",
    "        images, structured_data = images.to(device), structured_data.to(device)\n",
    "        features = combined_model(images, structured_data)\n",
    "        val_features.append(features)\n",
    "        val_labels.append(targets.numpy())\n",
    "\n",
    "    val_features = np.vstack(val_features)\n",
    "    val_labels = np.hstack(val_labels)\n",
    "    \n",
    "    val_preds = xgb_model.predict_proba(val_features)[:, 1]\n",
    "    auc = roc_auc_score(val_labels, val_preds)\n",
    "    print(f\"Epoch {epoch+1}/50, AUC: {auc}\")\n",
    "\n",
    "    # 保存最佳模型和MobileNetV1权重\n",
    "    if auc > best_auc:\n",
    "        best_auc = auc\n",
    "        xgb_model.save_model(f\"best_xgb_model.json\")\n",
    "        torch.save(mobilenet_model.state_dict(), f\"best_mobilenet_v1_weights.pth\")\n",
    "        print(f'Best model saved at epoch {epoch + 1}')\n",
    "\n",
    "# 测试过程\n",
    "test_dataset = CustomDataset(\n",
    "    csv_file='data/test-metadata-aligned.csv',\n",
    "    img_hdf5_file='data/test-image.hdf5',\n",
    "    transform=transform,\n",
    "    is_test=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# 加载保存的最佳 XGBoost 模型\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "xgb_model.load_model('best_xgb_model.json')\n",
    "\n",
    "# 加载保存的 MobileNetV1 模型权重\n",
    "mobilenet_model = MobileNetV1FeatureExtractor().to(device)\n",
    "mobilenet_model.load_state_dict(torch.load('best_mobilenet_v1_weights.pth'))\n",
    "mobilenet_model.eval()  # 设置模型为评估模式\n",
    "\n",
    "# 测试阶段生成预测\n",
    "test_results = []\n",
    "with torch.no_grad():\n",
    "    for images, structured_data, image_ids in test_loader:\n",
    "        images, structured_data = images.to(device), structured_data.to(device)\n",
    "        \n",
    "        # 使用加载的MobileNetV1提取图像特征\n",
    "        image_features = mobilenet_model(images).cpu().numpy()\n",
    "        \n",
    "        # 结合图像特征和结构化数据\n",
    "        combined_features = np.hstack((image_features, structured_data.cpu().numpy()))\n",
    "        \n",
    "        # 使用 XGBoost 模型进行预测\n",
    "        test_preds = xgb_model.predict_proba(combined_features)[:, 1]  # 预测概率\n",
    "\n",
    "        for i in range(len(image_ids)):\n",
    "            test_results.append({'isic_id': image_ids[i], 'target': test_preds[i]})\n",
    "\n",
    "# 保存测试结果\n",
    "test_df = pd.DataFrame(test_results)\n",
    "test_df.to_csv('test_predictions.csv', index=False)\n",
    "\n",
    "print(\"测试完成，预测结果已保存至 test_predictions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dfdad3b-38b6-4315-a013-d035871c0983",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-09-05T17:32:30.175229Z",
     "iopub.status.busy": "2024-09-05T17:32:30.174815Z",
     "iopub.status.idle": "2024-09-05T20:43:05.643739Z",
     "shell.execute_reply": "2024-09-05T20:43:05.643005Z",
     "shell.execute_reply.started": "2024-09-05T17:32:30.175205Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_282/2795419060.py:19: DtypeWarning: Columns (51,52) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  self.data = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: AUC = 0.9948\n",
      "Epoch 2: AUC = 0.9930\n",
      "Epoch 3: AUC = 0.9947\n",
      "Epoch 4: AUC = 0.9932\n",
      "Epoch 5: AUC = 0.9938\n",
      "Epoch 6: AUC = 0.9943\n",
      "Epoch 7: AUC = 0.9946\n",
      "Epoch 8: AUC = 0.9941\n",
      "Epoch 9: AUC = 0.9941\n",
      "Epoch 10: AUC = 0.9934\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Feature shape mismatch, expected: 1333, got 1323",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 249\u001b[0m\n\u001b[1;32m    247\u001b[0m images, structured_data \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), structured_data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    248\u001b[0m features \u001b[38;5;241m=\u001b[39m combined_model(images, structured_data)\n\u001b[0;32m--> 249\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m[:, \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    250\u001b[0m test_image_names\u001b[38;5;241m.\u001b[39mextend(image_ids\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m    251\u001b[0m test_predictions\u001b[38;5;241m.\u001b[39mextend(preds)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/xgboost/sklearn.py:1644\u001b[0m, in \u001b[0;36mXGBClassifier.predict_proba\u001b[0;34m(self, X, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[1;32m   1642\u001b[0m     class_prob \u001b[38;5;241m=\u001b[39m softmax(raw_predt, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   1643\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m class_prob\n\u001b[0;32m-> 1644\u001b[0m class_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1645\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1648\u001b[0m \u001b[43m    \u001b[49m\u001b[43miteration_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miteration_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _cls_predict_proba(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_, class_probs, np\u001b[38;5;241m.\u001b[39mvstack)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/xgboost/sklearn.py:1186\u001b[0m, in \u001b[0;36mXGBModel.predict\u001b[0;34m(self, X, output_margin, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_can_use_inplace_predict():\n\u001b[1;32m   1185\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1186\u001b[0m         predts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_booster\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace_predict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1188\u001b[0m \u001b[43m            \u001b[49m\u001b[43miteration_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miteration_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmargin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput_margin\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1194\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _is_cupy_alike(predts):\n\u001b[1;32m   1195\u001b[0m             \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcupy\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=import-error\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/xgboost/core.py:2520\u001b[0m, in \u001b[0;36mBooster.inplace_predict\u001b[0;34m(self, data, iteration_range, predict_type, missing, validate_features, base_margin, strict_shape)\u001b[0m\n\u001b[1;32m   2516\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   2517\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`shape` attribute is required when `validate_features` is True.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2518\u001b[0m         )\n\u001b[1;32m   2519\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_features() \u001b[38;5;241m!=\u001b[39m data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m-> 2520\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2521\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature shape mismatch, expected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_features()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2522\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2523\u001b[0m         )\n\u001b[1;32m   2525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_np_array_like(data):\n\u001b[1;32m   2526\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _ensure_np_dtype\n",
      "\u001b[0;31mValueError\u001b[0m: Feature shape mismatch, expected: 1333, got 1323"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "from PIL import Image\n",
    "import io\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "\n",
    "# 数据集类\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_hdf5_file, transform=None, is_test=False):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.img_hdf5_file = img_hdf5_file\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        \n",
    "        # 处理图像ID\n",
    "        self.image_ids = self.data['isic_id'].values\n",
    "        \n",
    "        # 处理结构化数据\n",
    "        if 'target' in self.data.columns:\n",
    "            self.structured_data = self.data.drop(columns=['isic_id', 'target'])\n",
    "        else:\n",
    "            self.structured_data = self.data.drop(columns=['isic_id'])\n",
    "        \n",
    "        # 特征工程：将字符串类型的列转换为数字\n",
    "        for col in self.structured_data.columns:\n",
    "            if self.structured_data[col].dtype == 'object':\n",
    "                self.structured_data[col] = self.structured_data[col].astype(str).str.extract('(\\d+)').astype(float)\n",
    "        \n",
    "        # 处理缺失值\n",
    "        self.structured_data = self.structured_data.fillna(0).values.astype(np.float32)\n",
    "        \n",
    "        # 特征标准化\n",
    "        self.scaler = StandardScaler()\n",
    "        self.structured_data = self.scaler.fit_transform(self.structured_data)\n",
    "        \n",
    "        if not is_test:\n",
    "            # 如果不是测试集，处理目标列\n",
    "            self.targets = self.data['target'].values.astype(np.float32)\n",
    "        else:\n",
    "            self.targets = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.img_hdf5_file, 'r') as hdf5_file:\n",
    "            image_key = self.image_ids[idx]\n",
    "            image_data = hdf5_file[image_key][()]\n",
    "            \n",
    "        image = Image.open(io.BytesIO(image_data))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        structured_data = torch.tensor(self.structured_data[idx], dtype=torch.float32)\n",
    "        if self.targets is not None:\n",
    "            target = torch.tensor(self.targets[idx], dtype=torch.float32)\n",
    "            return image, structured_data, target\n",
    "        else:\n",
    "            return image, structured_data, self.image_ids[idx]\n",
    "\n",
    "# 深度可分离卷积（Depthwise Separable Convolution）\n",
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, expand_ratio=1):\n",
    "        super(DepthwiseSeparableConv, self).__init__()\n",
    "        hidden_channels = in_channels * expand_ratio\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, hidden_channels, kernel_size=3, stride=stride, padding=1, groups=in_channels, bias=False),\n",
    "            nn.BatchNorm2d(hidden_channels),\n",
    "            nn.ReLU6(inplace=True),\n",
    "            nn.Conv2d(hidden_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "# MobileNetV2 的基本块（Block）\n",
    "class MobileNetV2Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride, expand_ratio):\n",
    "        super(MobileNetV2Block, self).__init__()\n",
    "        hidden_channels = in_channels * expand_ratio\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, hidden_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(hidden_channels),\n",
    "            nn.ReLU6(inplace=True),\n",
    "            DepthwiseSeparableConv(hidden_channels, out_channels, stride=stride, expand_ratio=expand_ratio)\n",
    "        )\n",
    "\n",
    "        self.use_residual = stride == 1 and in_channels == out_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_residual:\n",
    "            return x + self.block(x)\n",
    "        else:\n",
    "            return self.block(x)\n",
    "\n",
    "# MobileNetV2 模型\n",
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU6(inplace=True),\n",
    "            \n",
    "            MobileNetV2Block(32, 16, stride=1, expand_ratio=1),\n",
    "            MobileNetV2Block(16, 24, stride=2, expand_ratio=6),\n",
    "            MobileNetV2Block(24, 24, stride=1, expand_ratio=6),\n",
    "            MobileNetV2Block(24, 32, stride=2, expand_ratio=6),\n",
    "            MobileNetV2Block(32, 32, stride=1, expand_ratio=6),\n",
    "            MobileNetV2Block(32, 64, stride=2, expand_ratio=6),\n",
    "            MobileNetV2Block(64, 64, stride=1, expand_ratio=6),\n",
    "            MobileNetV2Block(64, 96, stride=1, expand_ratio=6),\n",
    "            MobileNetV2Block(96, 96, stride=1, expand_ratio=6),\n",
    "            MobileNetV2Block(96, 160, stride=2, expand_ratio=6),\n",
    "            MobileNetV2Block(160, 160, stride=1, expand_ratio=6),\n",
    "            MobileNetV2Block(160, 320, stride=1, expand_ratio=6),\n",
    "\n",
    "            nn.Conv2d(320, 1280, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(1280),\n",
    "            nn.ReLU6(inplace=True),\n",
    "\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(1280, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# 定义结合MobileNetV2特征和结构化特征的XGBoost模型\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, mobilenet_model, structured_data_dim):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.mobilenet = mobilenet_model\n",
    "        self.structured_data_dim = structured_data_dim\n",
    "\n",
    "    def forward(self, image, structured_data):\n",
    "        image_features = self.mobilenet(image).detach().cpu().numpy()\n",
    "        combined_features = np.hstack((image_features, structured_data.detach().cpu().numpy()))\n",
    "        return combined_features\n",
    "\n",
    "# 判断是否有可用的GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 数据变换\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((15, 15)),  # 修改为15x15图像尺寸\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 加载数据集\n",
    "train_dataset = CustomDataset(\n",
    "    csv_file='data/train-metadata.csv',\n",
    "    img_hdf5_file='data/train-image.hdf5',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# 划分训练集和验证集\n",
    "train_size = int(0.9 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_set, val_set = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=32, shuffle=False)\n",
    "\n",
    "# 初始化模型\n",
    "mobilenet_model = MobileNetV2(num_classes=1280).to(device)\n",
    "combined_model = CombinedModel(mobilenet_model, structured_data_dim=train_set[0][1].shape[0])\n",
    "\n",
    "# XGBoost分类器\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', n_estimators=100)\n",
    "\n",
    "# 训练与验证\n",
    "best_auc = 0\n",
    "for epoch in range(10):  # 这里只用1个epoch演示，实际应使用更多\n",
    "    train_features = []\n",
    "    train_labels = []\n",
    "\n",
    "    # 训练过程\n",
    "    for images, structured_data, targets in train_loader:\n",
    "        images, structured_data = images.to(device), structured_data.to(device)\n",
    "        features = combined_model(images, structured_data)\n",
    "        train_features.append(features)\n",
    "        train_labels.append(targets.numpy())\n",
    "    \n",
    "    train_features = np.vstack(train_features)\n",
    "    train_labels = np.hstack(train_labels)\n",
    "\n",
    "    xgb_model.fit(train_features, train_labels)\n",
    "\n",
    "    # 验证过程\n",
    "    val_features = []\n",
    "    val_labels = []\n",
    "\n",
    "    for images, structured_data, targets in val_loader:\n",
    "        images, structured_data = images.to(device), structured_data.to(device)\n",
    "        features = combined_model(images, structured_data)\n",
    "        val_features.append(features)\n",
    "        val_labels.append(targets.numpy())\n",
    "\n",
    "    val_features = np.vstack(val_features)\n",
    "    val_labels = np.hstack(val_labels)\n",
    "    \n",
    "    val_preds = xgb_model.predict_proba(val_features)[:, 1]\n",
    "    auc = roc_auc_score(val_labels, val_preds)\n",
    "    print(f\"Epoch {epoch+1}: AUC = {auc:.4f}\")\n",
    "\n",
    "    # 保存最佳模型\n",
    "    if auc > best_auc:\n",
    "        best_auc = auc\n",
    "        torch.save(combined_model.state_dict(), 'best_combined_model.pth')\n",
    "        xgb_model.save_model('best_xgb_model.json')\n",
    "\n",
    "# 测试过程\n",
    "test_dataset = CustomDataset(\n",
    "    csv_file='data/test-metadata.csv',\n",
    "    img_hdf5_file='data/test-image.hdf5',\n",
    "    transform=transform,\n",
    "    is_test=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# 加载最佳模型\n",
    "combined_model.load_state_dict(torch.load('best_combined_model.pth'))\n",
    "xgb_model.load_model('best_xgb_model.json')\n",
    "\n",
    "test_image_names = []\n",
    "test_predictions = []\n",
    "\n",
    "for images, structured_data, image_ids in test_loader:\n",
    "    images, structured_data = images.to(device), structured_data.to(device)\n",
    "    features = combined_model(images, structured_data)\n",
    "    preds = xgb_model.predict_proba(features)[:, 1]\n",
    "    test_image_names.extend(image_ids.numpy())\n",
    "    test_predictions.extend(preds)\n",
    "\n",
    "# 保存测试结果\n",
    "results = pd.DataFrame({'image_id': test_image_names, 'prediction': test_predictions})\n",
    "results.to_csv('test_predictions.csv', index=False)\n",
    "\n",
    "print(\"Testing completed. Predictions saved to test_predictions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a2c650-5c0e-45c0-a0cc-0167f50ad9a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f526490c-36cc-4cfb-b3b2-a6e37b816088",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-09-06T13:19:11.915639Z",
     "iopub.status.busy": "2024-09-06T13:19:11.915199Z",
     "iopub.status.idle": "2024-09-06T13:28:02.362438Z",
     "shell.execute_reply": "2024-09-06T13:28:02.361611Z",
     "shell.execute_reply.started": "2024-09-06T13:19:11.915610Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 197\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, structured_data, targets \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m    196\u001b[0m     images, structured_data \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), structured_data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 197\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mcombined_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstructured_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     train_features\u001b[38;5;241m.\u001b[39mappend(features)\n\u001b[1;32m    199\u001b[0m     train_labels\u001b[38;5;241m.\u001b[39mappend(targets\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 153\u001b[0m, in \u001b[0;36mCombinedModel.forward\u001b[0;34m(self, image, structured_data)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, image, structured_data):\n\u001b[0;32m--> 153\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmobilenet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    154\u001b[0m     combined_features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack((image_features, structured_data\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()))\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m combined_features\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 140\u001b[0m, in \u001b[0;36mMobileNetV2.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 140\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    142\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 104\u001b[0m, in \u001b[0;36mMobileNetV2Block.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_residual:\n\u001b[0;32m--> 104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    106\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock(x)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 85\u001b[0m, in \u001b[0;36mDepthwiseSeparableConv.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/activation.py:234\u001b[0m, in \u001b[0;36mHardtanh.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhardtanh\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/functional.py:1549\u001b[0m, in \u001b[0;36mhardtanh\u001b[0;34m(input, min_val, max_val, inplace)\u001b[0m\n\u001b[1;32m   1547\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(hardtanh, (\u001b[38;5;28minput\u001b[39m,), \u001b[38;5;28minput\u001b[39m, min_val\u001b[38;5;241m=\u001b[39mmin_val, max_val\u001b[38;5;241m=\u001b[39mmax_val, inplace\u001b[38;5;241m=\u001b[39minplace)\n\u001b[1;32m   1548\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m-> 1549\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhardtanh_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1550\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1551\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mhardtanh(\u001b[38;5;28minput\u001b[39m, min_val, max_val)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "from PIL import Image\n",
    "import io\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "\n",
    "# 数据集类\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_hdf5_file, transform=None, is_test=False):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.img_hdf5_file = img_hdf5_file\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        \n",
    "        # 处理图像ID\n",
    "        self.image_ids = self.data['isic_id'].values\n",
    "        \n",
    "        # 处理结构化数据\n",
    "        if 'target' in self.data.columns:\n",
    "            self.structured_data = self.data.drop(columns=['isic_id', 'target'])\n",
    "        else:\n",
    "            self.structured_data = self.data.drop(columns=['isic_id'])\n",
    "        \n",
    "        # 特征工程：将字符串类型的列转换为数字\n",
    "        for col in self.structured_data.columns:\n",
    "            if self.structured_data[col].dtype == 'object':\n",
    "                self.structured_data[col] = self.structured_data[col].astype(str).str.extract('(\\d+)').astype(float)\n",
    "        \n",
    "        # 处理缺失值\n",
    "        self.structured_data = self.structured_data.fillna(0).values.astype(np.float32)\n",
    "        \n",
    "        # 特征标准化\n",
    "        self.scaler = StandardScaler()\n",
    "        self.structured_data = self.scaler.fit_transform(self.structured_data)\n",
    "        \n",
    "        if not is_test:\n",
    "            # 如果不是测试集，处理目标列\n",
    "            self.targets = self.data['target'].values.astype(np.float32)\n",
    "        else:\n",
    "            self.targets = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.img_hdf5_file, 'r') as hdf5_file:\n",
    "            image_key = self.image_ids[idx]\n",
    "            image_data = hdf5_file[image_key][()]\n",
    "            \n",
    "        image = Image.open(io.BytesIO(image_data))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        structured_data = torch.tensor(self.structured_data[idx], dtype=torch.float32)\n",
    "        if self.targets is not None:\n",
    "            target = torch.tensor(self.targets[idx], dtype=torch.float32)\n",
    "            return image, structured_data, target\n",
    "        else:\n",
    "            return image, structured_data, self.image_ids[idx]\n",
    "\n",
    "# 深度可分离卷积（Depthwise Separable Convolution）\n",
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, expand_ratio=1):\n",
    "        super(DepthwiseSeparableConv, self).__init__()\n",
    "        hidden_channels = in_channels * expand_ratio\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, hidden_channels, kernel_size=3, stride=stride, padding=1, groups=in_channels, bias=False),\n",
    "            nn.BatchNorm2d(hidden_channels),\n",
    "            nn.ReLU6(inplace=True),\n",
    "            nn.Conv2d(hidden_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "# MobileNetV2 的基本块（Block）\n",
    "class MobileNetV2Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride, expand_ratio):\n",
    "        super(MobileNetV2Block, self).__init__()\n",
    "        hidden_channels = in_channels * expand_ratio\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, hidden_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(hidden_channels),\n",
    "            nn.ReLU6(inplace=True),\n",
    "            DepthwiseSeparableConv(hidden_channels, out_channels, stride=stride, expand_ratio=expand_ratio)\n",
    "        )\n",
    "\n",
    "        self.use_residual = stride == 1 and in_channels == out_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_residual:\n",
    "            return x + self.block(x)\n",
    "        else:\n",
    "            return self.block(x)\n",
    "\n",
    "# MobileNetV2 模型\n",
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU6(inplace=True),\n",
    "            \n",
    "            MobileNetV2Block(32, 16, stride=1, expand_ratio=1),\n",
    "            MobileNetV2Block(16, 24, stride=2, expand_ratio=6),\n",
    "            MobileNetV2Block(24, 24, stride=1, expand_ratio=6),\n",
    "            MobileNetV2Block(24, 32, stride=2, expand_ratio=6),\n",
    "            MobileNetV2Block(32, 32, stride=1, expand_ratio=6),\n",
    "            MobileNetV2Block(32, 64, stride=2, expand_ratio=6),\n",
    "            MobileNetV2Block(64, 64, stride=1, expand_ratio=6),\n",
    "            MobileNetV2Block(64, 96, stride=1, expand_ratio=6),\n",
    "            MobileNetV2Block(96, 96, stride=1, expand_ratio=6),\n",
    "            MobileNetV2Block(96, 160, stride=2, expand_ratio=6),\n",
    "            MobileNetV2Block(160, 160, stride=1, expand_ratio=6),\n",
    "            MobileNetV2Block(160, 320, stride=1, expand_ratio=6),\n",
    "\n",
    "            nn.Conv2d(320, 1280, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(1280),\n",
    "            nn.ReLU6(inplace=True),\n",
    "\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(1280, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# 定义结合MobileNetV2特征和结构化特征的XGBoost模型\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, mobilenet_model, structured_data_dim):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.mobilenet = mobilenet_model\n",
    "        self.structured_data_dim = structured_data_dim\n",
    "\n",
    "    def forward(self, image, structured_data):\n",
    "        image_features = self.mobilenet(image).detach().cpu().numpy()\n",
    "        combined_features = np.hstack((image_features, structured_data.detach().cpu().numpy()))\n",
    "        return combined_features\n",
    "\n",
    "# 判断是否有可用的GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 数据变换\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((15, 15)),  # 修改为15x15图像尺寸\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 加载数据集\n",
    "train_dataset = CustomDataset(\n",
    "    csv_file='data/train_metadata_aligned.csv',\n",
    "    img_hdf5_file='data/train-image.hdf5',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# 划分训练集和验证集\n",
    "train_size = int(0.9 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_set, val_set = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=32, shuffle=False)\n",
    "\n",
    "# 初始化模型\n",
    "mobilenet_model = MobileNetV2(num_classes=1280).to(device)\n",
    "combined_model = CombinedModel(mobilenet_model, structured_data_dim=train_set[0][1].shape[0])\n",
    "\n",
    "# XGBoost分类器\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', n_estimators=100)\n",
    "\n",
    "# 训练与验证\n",
    "best_auc = 0\n",
    "for epoch in range(20):  # 这里只用1个epoch演示，实际应使用更多\n",
    "    train_features = []\n",
    "    train_labels = []\n",
    "\n",
    "    # 训练过程\n",
    "    for images, structured_data, targets in train_loader:\n",
    "        images, structured_data = images.to(device), structured_data.to(device)\n",
    "        features = combined_model(images, structured_data)\n",
    "        train_features.append(features)\n",
    "        train_labels.append(targets.numpy())\n",
    "    \n",
    "    train_features = np.vstack(train_features)\n",
    "    train_labels = np.hstack(train_labels)\n",
    "\n",
    "    xgb_model.fit(train_features, train_labels)\n",
    "\n",
    "    # 验证过程\n",
    "    val_features = []\n",
    "    val_labels = []\n",
    "\n",
    "    for images, structured_data, targets in val_loader:\n",
    "        images, structured_data = images.to(device), structured_data.to(device)\n",
    "        features = combined_model(images, structured_data)\n",
    "        val_features.append(features)\n",
    "        val_labels.append(targets.numpy())\n",
    "\n",
    "    val_features = np.vstack(val_features)\n",
    "    val_labels = np.hstack(val_labels)\n",
    "    \n",
    "    val_preds = xgb_model.predict_proba(val_features)[:, 1]\n",
    "    auc = roc_auc_score(val_labels, val_preds)\n",
    "    print(f\"Epoch {epoch+1}: AUC = {auc:.4f}\")\n",
    "\n",
    "    # 保存最佳模型\n",
    "    if auc > best_auc:\n",
    "        best_auc = auc\n",
    "        torch.save(combined_model.state_dict(), '0906best_combined_model.pth')\n",
    "        xgb_model.save_model('0906best_xgb_model.json')\n",
    "\n",
    "\n",
    "# 测试过程\n",
    "test_dataset = CustomDataset(\n",
    "    csv_file='data/test-metadata.csv',\n",
    "    img_hdf5_file='data/test-image.hdf5',\n",
    "    transform=transform,\n",
    "    is_test=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# 加载最佳模型\n",
    "combined_model.load_state_dict(torch.load('0906best_combined_model.pth'))\n",
    "xgb_model.load_model('0906best_xgb_model.json')\n",
    "\n",
    "test_image_names = []\n",
    "test_predictions = []\n",
    "\n",
    "for images, structured_data, image_ids in test_loader:\n",
    "    images, structured_data = images.to(device), structured_data.to(device)\n",
    "    features = combined_model(images, structured_data)\n",
    "    preds = xgb_model.predict_proba(features)[:, 1]\n",
    "    test_image_names.extend(image_ids)  # 这里不需要调用 numpy()\n",
    "    test_predictions.extend(preds)\n",
    "\n",
    "# 保存测试结果\n",
    "results = pd.DataFrame({'image_id': test_image_names, 'prediction': test_predictions})\n",
    "results.to_csv('test_predictions.csv', index=False)\n",
    "\n",
    "print(\"Testing completed. Predictions saved to test_predictions.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92601812-e8de-484e-9e0b-df5be9913642",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-09-06T10:17:25.567881Z",
     "iopub.status.busy": "2024-09-06T10:17:25.567452Z",
     "iopub.status.idle": "2024-09-06T10:17:40.877111Z",
     "shell.execute_reply": "2024-09-06T10:17:40.876569Z",
     "shell.execute_reply.started": "2024-09-06T10:17:25.567856Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing completed. Predictions saved to test_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "from PIL import Image\n",
    "import io\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "\n",
    "# 数据集类\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_hdf5_file, transform=None, is_test=False):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.img_hdf5_file = img_hdf5_file\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        \n",
    "        # 处理图像ID\n",
    "        self.image_ids = self.data['isic_id'].values\n",
    "        \n",
    "        # 处理结构化数据\n",
    "        if 'target' in self.data.columns:\n",
    "            self.structured_data = self.data.drop(columns=['isic_id', 'target'])\n",
    "        else:\n",
    "            self.structured_data = self.data.drop(columns=['isic_id'])\n",
    "        \n",
    "        # 特征工程：将字符串类型的列转换为数字\n",
    "        for col in self.structured_data.columns:\n",
    "            if self.structured_data[col].dtype == 'object':\n",
    "                self.structured_data[col] = self.structured_data[col].astype(str).str.extract('(\\d+)').astype(float)\n",
    "        \n",
    "        # 处理缺失值\n",
    "        self.structured_data = self.structured_data.fillna(0).values.astype(np.float32)\n",
    "        \n",
    "        # 特征标准化\n",
    "        self.scaler = StandardScaler()\n",
    "        self.structured_data = self.scaler.fit_transform(self.structured_data)\n",
    "        \n",
    "        if not is_test:\n",
    "            # 如果不是测试集，处理目标列\n",
    "            self.targets = self.data['target'].values.astype(np.float32)\n",
    "        else:\n",
    "            self.targets = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.img_hdf5_file, 'r') as hdf5_file:\n",
    "            image_key = self.image_ids[idx]\n",
    "            image_data = hdf5_file[image_key][()]\n",
    "            \n",
    "        image = Image.open(io.BytesIO(image_data))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        structured_data = torch.tensor(self.structured_data[idx], dtype=torch.float32)\n",
    "        if self.targets is not None:\n",
    "            target = torch.tensor(self.targets[idx], dtype=torch.float32)\n",
    "            return image, structured_data, target\n",
    "        else:\n",
    "            return image, structured_data, self.image_ids[idx]\n",
    "\n",
    "# 深度可分离卷积（Depthwise Separable Convolution）\n",
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, expand_ratio=1):\n",
    "        super(DepthwiseSeparableConv, self).__init__()\n",
    "        hidden_channels = in_channels * expand_ratio\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, hidden_channels, kernel_size=3, stride=stride, padding=1, groups=in_channels, bias=False),\n",
    "            nn.BatchNorm2d(hidden_channels),\n",
    "            nn.ReLU6(inplace=True),\n",
    "            nn.Conv2d(hidden_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "# MobileNetV2 的基本块（Block）\n",
    "class MobileNetV2Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride, expand_ratio):\n",
    "        super(MobileNetV2Block, self).__init__()\n",
    "        hidden_channels = in_channels * expand_ratio\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, hidden_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(hidden_channels),\n",
    "            nn.ReLU6(inplace=True),\n",
    "            DepthwiseSeparableConv(hidden_channels, out_channels, stride=stride, expand_ratio=expand_ratio)\n",
    "        )\n",
    "\n",
    "        self.use_residual = stride == 1 and in_channels == out_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_residual:\n",
    "            return x + self.block(x)\n",
    "        else:\n",
    "            return self.block(x)\n",
    "\n",
    "# MobileNetV2 模型\n",
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU6(inplace=True),\n",
    "            \n",
    "            MobileNetV2Block(32, 16, stride=1, expand_ratio=1),\n",
    "            MobileNetV2Block(16, 24, stride=2, expand_ratio=6),\n",
    "            MobileNetV2Block(24, 24, stride=1, expand_ratio=6),\n",
    "            MobileNetV2Block(24, 32, stride=2, expand_ratio=6),\n",
    "            MobileNetV2Block(32, 32, stride=1, expand_ratio=6),\n",
    "            MobileNetV2Block(32, 64, stride=2, expand_ratio=6),\n",
    "            MobileNetV2Block(64, 64, stride=1, expand_ratio=6),\n",
    "            MobileNetV2Block(64, 96, stride=1, expand_ratio=6),\n",
    "            MobileNetV2Block(96, 96, stride=1, expand_ratio=6),\n",
    "            MobileNetV2Block(96, 160, stride=2, expand_ratio=6),\n",
    "            MobileNetV2Block(160, 160, stride=1, expand_ratio=6),\n",
    "            MobileNetV2Block(160, 320, stride=1, expand_ratio=6),\n",
    "\n",
    "            nn.Conv2d(320, 1280, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(1280),\n",
    "            nn.ReLU6(inplace=True),\n",
    "\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(1280, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# 定义结合MobileNetV2特征和结构化特征的XGBoost模型\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, mobilenet_model, structured_data_dim):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.mobilenet = mobilenet_model\n",
    "        self.structured_data_dim = structured_data_dim\n",
    "\n",
    "    def forward(self, image, structured_data):\n",
    "        image_features = self.mobilenet(image).detach().cpu().numpy()\n",
    "        combined_features = np.hstack((image_features, structured_data.detach().cpu().numpy()))\n",
    "        return combined_features\n",
    "\n",
    "# 判断是否有可用的GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 数据变换\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((15, 15)),  # 修改为15x15图像尺寸\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 加载数据集\n",
    "train_dataset = CustomDataset(\n",
    "    csv_file='data/train_metadata_aligned.csv',\n",
    "    img_hdf5_file='data/train-image.hdf5',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# 划分训练集和验证集\n",
    "train_size = int(0.9 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_set, val_set = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=32, shuffle=False)\n",
    "\n",
    "# 初始化模型\n",
    "mobilenet_model = MobileNetV2(num_classes=1280).to(device)\n",
    "combined_model = CombinedModel(mobilenet_model, structured_data_dim=train_set[0][1].shape[0])\n",
    "\n",
    "# XGBoost分类器\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', n_estimators=100)\n",
    "\n",
    "# 测试过程\n",
    "test_dataset = CustomDataset(\n",
    "    csv_file='data/test-metadata.csv',\n",
    "    img_hdf5_file='data/test-image.hdf5',\n",
    "    transform=transform,\n",
    "    is_test=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# 加载最佳模型\n",
    "combined_model.load_state_dict(torch.load('0906best_combined_model.pth'))\n",
    "xgb_model.load_model('0906best_xgb_model.json')\n",
    "\n",
    "test_image_names = []\n",
    "test_predictions = []\n",
    "\n",
    "for images, structured_data, image_ids in test_loader:\n",
    "    images, structured_data = images.to(device), structured_data.to(device)\n",
    "    features = combined_model(images, structured_data)\n",
    "    preds = xgb_model.predict_proba(features)[:, 1]\n",
    "    test_image_names.extend(image_ids)  # 这里不需要调用 numpy()\n",
    "    test_predictions.extend(preds)\n",
    "\n",
    "# 保存测试结果\n",
    "results = pd.DataFrame({'image_id': test_image_names, 'prediction': test_predictions})\n",
    "results.to_csv('test_predictions.csv', index=False)\n",
    "\n",
    "print(\"Testing completed. Predictions saved to test_predictions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59451fb5-22df-4452-a160-40e05fd524d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d76453a-1561-4f30-879e-2c10a68ef2c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1a0795-46c6-44d8-a96c-62fdfb91519b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "from PIL import Image\n",
    "import io\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "\n",
    "# 数据集类\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_hdf5_file, transform=None, is_test=False):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.img_hdf5_file = img_hdf5_file\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        \n",
    "        # 处理图像ID\n",
    "        self.image_ids = self.data['isic_id'].values\n",
    "        \n",
    "        # 处理结构化数据\n",
    "        if 'target' in self.data.columns:\n",
    "            self.structured_data = self.data.drop(columns=['isic_id', 'target'])\n",
    "        else:\n",
    "            self.structured_data = self.data.drop(columns=['isic_id'])\n",
    "        \n",
    "        # 特征工程：将字符串类型的列转换为数字\n",
    "        for col in self.structured_data.columns:\n",
    "            if self.structured_data[col].dtype == 'object':\n",
    "                self.structured_data[col] = self.structured_data[col].astype(str).str.extract('(\\d+)').astype(float)\n",
    "        \n",
    "        # 处理缺失值\n",
    "        self.structured_data = self.structured_data.fillna(0).values.astype(np.float32)\n",
    "        \n",
    "        # 特征标准化\n",
    "        self.scaler = StandardScaler()\n",
    "        self.structured_data = self.scaler.fit_transform(self.structured_data)\n",
    "        \n",
    "        if not is_test:\n",
    "            # 如果不是测试集，处理目标列\n",
    "            self.targets = self.data['target'].values.astype(np.float32)\n",
    "        else:\n",
    "            self.targets = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.img_hdf5_file, 'r') as hdf5_file:\n",
    "            image_key = self.image_ids[idx]\n",
    "            image_data = hdf5_file[image_key][()]\n",
    "            \n",
    "        image = Image.open(io.BytesIO(image_data))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        structured_data = torch.tensor(self.structured_data[idx], dtype=torch.float32)\n",
    "        if self.targets is not None:\n",
    "            target = torch.tensor(self.targets[idx], dtype=torch.float32)\n",
    "            return image, structured_data, target\n",
    "        else:\n",
    "            return image, structured_data, self.image_ids[idx]\n",
    "\n",
    "# 深度可分离卷积（Depthwise Separable Convolution）\n",
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, expand_ratio=1):\n",
    "        super(DepthwiseSeparableConv, self).__init__()\n",
    "        hidden_channels = in_channels * expand_ratio\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, hidden_channels, kernel_size=3, stride=stride, padding=1, groups=in_channels, bias=False),\n",
    "            nn.BatchNorm2d(hidden_channels),\n",
    "            nn.ReLU6(inplace=True),\n",
    "            nn.Conv2d(hidden_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "# MobileNetV2 的基本块（Block）\n",
    "class MobileNetV2Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride, expand_ratio):\n",
    "        super(MobileNetV2Block, self).__init__()\n",
    "        hidden_channels = in_channels * expand_ratio\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, hidden_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(hidden_channels),\n",
    "            nn.ReLU6(inplace=True),\n",
    "            DepthwiseSeparableConv(hidden_channels, out_channels, stride=stride, expand_ratio=expand_ratio)\n",
    "        )\n",
    "\n",
    "        self.use_residual = stride == 1 and in_channels == out_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_residual:\n",
    "            return x + self.block(x)\n",
    "        else:\n",
    "            return self.block(x)\n",
    "\n",
    "# MobileNetV2 模型\n",
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU6(inplace=True),\n",
    "            \n",
    "            MobileNetV2Block(32, 16, stride=1, expand_ratio=1),\n",
    "            MobileNetV2Block(16, 24, stride=2, expand_ratio=6),\n",
    "            MobileNetV2Block(24, 24, stride=1, expand_ratio=6),\n",
    "            MobileNetV2Block(24, 32, stride=2, expand_ratio=6),\n",
    "            MobileNetV2Block(32, 32, stride=1, expand_ratio=6),\n",
    "            MobileNetV2Block(32, 64, stride=2, expand_ratio=6),\n",
    "            MobileNetV2Block(64, 64, stride=1, expand_ratio=6),\n",
    "            MobileNetV2Block(64, 96, stride=1, expand_ratio=6),\n",
    "            MobileNetV2Block(96, 96, stride=1, expand_ratio=6),\n",
    "            MobileNetV2Block(96, 160, stride=2, expand_ratio=6),\n",
    "            MobileNetV2Block(160, 160, stride=1, expand_ratio=6),\n",
    "            MobileNetV2Block(160, 320, stride=1, expand_ratio=6),\n",
    "\n",
    "            nn.Conv2d(320, 1280, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(1280),\n",
    "            nn.ReLU6(inplace=True),\n",
    "\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(1280, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# 定义结合MobileNetV2特征和结构化特征的XGBoost模型\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, mobilenet_model, structured_data_dim):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.mobilenet = mobilenet_model\n",
    "        self.structured_data_dim = structured_data_dim\n",
    "\n",
    "    def forward(self, image, structured_data):\n",
    "        image_features = self.mobilenet(image).detach().cpu().numpy()\n",
    "        combined_features = np.hstack((image_features, structured_data.detach().cpu().numpy()))\n",
    "        return combined_features\n",
    "\n",
    "# 判断是否有可用的GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 数据变换\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((15, 15)),  # 修改为15x15图像尺寸\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 加载数据集\n",
    "train_dataset = CustomDataset(\n",
    "    csv_file='data/train_metadata_aligned.csv',\n",
    "    img_hdf5_file='data/train-image.hdf5',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# 划分训练集和验证集\n",
    "train_size = int(0.9 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_set, val_set = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=32, shuffle=False)\n",
    "\n",
    "# 初始化模型\n",
    "mobilenet_model = MobileNetV2(num_classes=1280).to(device)\n",
    "combined_model = CombinedModel(mobilenet_model, structured_data_dim=train_set[0][1].shape[0])\n",
    "\n",
    "# XGBoost分类器\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', n_estimators=100)\n",
    "\n",
    "# 测试过程\n",
    "test_dataset = CustomDataset(\n",
    "    csv_file='data/test-metadata.csv',\n",
    "    img_hdf5_file='data/test-image.hdf5',\n",
    "    transform=transform,\n",
    "    is_test=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# 加载最佳模型\n",
    "combined_model.load_state_dict(torch.load('0906best_combined_model.pth'))\n",
    "xgb_model.load_model('0906best_xgb_model.json')\n",
    "\n",
    "test_image_names = []\n",
    "test_predictions = []\n",
    "\n",
    "for images, structured_data, image_ids in test_loader:\n",
    "    images, structured_data = images.to(device), structured_data.to(device)\n",
    "    features = combined_model(images, structured_data)\n",
    "    preds = xgb_model.predict_proba(features)[:, 1]\n",
    "    test_image_names.extend(image_ids)  # 这里不需要调用 numpy()\n",
    "    test_predictions.extend(preds)\n",
    "\n",
    "# 保存测试结果\n",
    "results = pd.DataFrame({'image_id': test_image_names, 'prediction': test_predictions})\n",
    "results.to_csv('test_predictions.csv', index=False)\n",
    "\n",
    "print(\"Testing completed. Predictions saved to test_predictions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64617e10-1a19-49b1-8b3f-c0da79eac54f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cee9ef92-fbb4-49c9-86ed-f6dcc205f514",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-09-06T14:32:17.445604Z",
     "iopub.status.busy": "2024-09-06T14:32:17.445066Z",
     "iopub.status.idle": "2024-09-06T17:52:36.746994Z",
     "shell.execute_reply": "2024-09-06T17:52:36.746189Z",
     "shell.execute_reply.started": "2024-09-06T14:32:17.445581Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: AUC = 0.9169\n",
      "Epoch 2: AUC = 0.9343\n",
      "Epoch 3: AUC = 0.9232\n",
      "Epoch 4: AUC = 0.9235\n",
      "Epoch 5: AUC = 0.9044\n",
      "Epoch 6: AUC = 0.9291\n",
      "Epoch 7: AUC = 0.9349\n",
      "Epoch 8: AUC = 0.9116\n",
      "Epoch 9: AUC = 0.9090\n",
      "Epoch 10: AUC = 0.9216\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 139\u001b[0m\n\u001b[1;32m    136\u001b[0m train_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# 训练过程\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, structured_data, targets \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m    140\u001b[0m     images, structured_data \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), structured_data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    141\u001b[0m     features \u001b[38;5;241m=\u001b[39m combined_model(images, structured_data)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/utils/data/dataset.py:419\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/utils/data/dataset.py:419\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[0;32mIn[7], line 57\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m h5py\u001b[38;5;241m.\u001b[39mFile(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_hdf5_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m hdf5_file:\n\u001b[1;32m     56\u001b[0m     image_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_ids[idx]\n\u001b[0;32m---> 57\u001b[0m     image_data \u001b[38;5;241m=\u001b[39m \u001b[43mhdf5_file\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimage_key\u001b[49m\u001b[43m]\u001b[49m[()]\n\u001b[1;32m     59\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(io\u001b[38;5;241m.\u001b[39mBytesIO(image_data))\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/h5py/_hl/group.py:357\u001b[0m, in \u001b[0;36mGroup.__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid HDF5 object reference\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(name, (\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mstr\u001b[39m)):\n\u001b[0;32m--> 357\u001b[0m     oid \u001b[38;5;241m=\u001b[39m \u001b[43mh5o\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_e\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccessing a group is done with bytes or str, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    360\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(name)))\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5o.pyx:241\u001b[0m, in \u001b[0;36mh5py.h5o.open\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5i.pyx:43\u001b[0m, in \u001b[0;36mh5py.h5i.wrap_identifier\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:404\u001b[0m, in \u001b[0;36mparent\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "from PIL import Image\n",
    "import io\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "\n",
    "# 数据集类\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_hdf5_file, transform=None, is_test=False):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.img_hdf5_file = img_hdf5_file\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        \n",
    "        # 处理图像ID\n",
    "        self.image_ids = self.data['isic_id'].values\n",
    "        \n",
    "        # 处理结构化数据\n",
    "        if 'target' in self.data.columns:\n",
    "            self.structured_data = self.data.drop(columns=['isic_id', 'target'])\n",
    "        else:\n",
    "            self.structured_data = self.data.drop(columns=['isic_id'])\n",
    "        \n",
    "        # 特征工程：将字符串类型的列转换为数字\n",
    "        for col in self.structured_data.columns:\n",
    "            if self.structured_data[col].dtype == 'object':\n",
    "                self.structured_data[col] = self.structured_data[col].astype(str).str.extract('(\\d+)').astype(float)\n",
    "        \n",
    "        # 处理缺失值\n",
    "        self.structured_data = self.structured_data.fillna(0).values.astype(np.float32)\n",
    "        \n",
    "        # 特征标准化\n",
    "        self.scaler = StandardScaler()\n",
    "        self.structured_data = self.scaler.fit_transform(self.structured_data)\n",
    "        \n",
    "        if not is_test:\n",
    "            # 如果不是测试集，处理目标列\n",
    "            self.targets = self.data['target'].values.astype(np.float32)\n",
    "        else:\n",
    "            self.targets = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.img_hdf5_file, 'r') as hdf5_file:\n",
    "            image_key = self.image_ids[idx]\n",
    "            image_data = hdf5_file[image_key][()]\n",
    "            \n",
    "        image = Image.open(io.BytesIO(image_data))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        structured_data = torch.tensor(self.structured_data[idx], dtype=torch.float32)\n",
    "        if self.targets is not None:\n",
    "            target = torch.tensor(self.targets[idx], dtype=torch.float32)\n",
    "            return image, structured_data, target\n",
    "        else:\n",
    "            return image, structured_data, self.image_ids[idx]\n",
    "\n",
    "# 定义SimpleCNN模型\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=1280):  # 输出1280与之前MobileNetV2保持一致\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc = nn.Linear(64 * 1 * 1, num_classes)  # 假设最终特征图大小为1x1\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(x.size(0), -1)  # 展平\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# 定义结合SimpleCNN特征和结构化特征的XGBoost模型\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, simplecnn_model, structured_data_dim):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.simplecnn = simplecnn_model\n",
    "        self.structured_data_dim = structured_data_dim\n",
    "\n",
    "    def forward(self, image, structured_data):\n",
    "        image_features = self.simplecnn(image).detach().cpu().numpy()\n",
    "        combined_features = np.hstack((image_features, structured_data.detach().cpu().numpy()))\n",
    "        return combined_features\n",
    "\n",
    "\n",
    "# 判断是否有可用的GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 数据变换\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((15, 15)),  # 修改为15x15图像尺寸\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 加载数据集\n",
    "train_dataset = CustomDataset(\n",
    "    csv_file='data/train_metadata_aligned.csv',\n",
    "    img_hdf5_file='data/train-image.hdf5',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# 划分训练集和验证集\n",
    "train_size = int(0.9 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_set, val_set = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=32, shuffle=False)\n",
    "\n",
    "# 初始化模型\n",
    "mobilenet_model = MobileNetV2(num_classes=1280).to(device)\n",
    "combined_model = CombinedModel(mobilenet_model, structured_data_dim=train_set[0][1].shape[0])\n",
    "\n",
    "# XGBoost分类器\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', n_estimators=100)\n",
    "\n",
    "# 训练与验证\n",
    "best_auc = 0\n",
    "for epoch in range(20):  # 这里只用1个epoch演示，实际应使用更多\n",
    "    train_features = []\n",
    "    train_labels = []\n",
    "\n",
    "    # 训练过程\n",
    "    for images, structured_data, targets in train_loader:\n",
    "        images, structured_data = images.to(device), structured_data.to(device)\n",
    "        features = combined_model(images, structured_data)\n",
    "        train_features.append(features)\n",
    "        train_labels.append(targets.numpy())\n",
    "    \n",
    "    train_features = np.vstack(train_features)\n",
    "    train_labels = np.hstack(train_labels)\n",
    "\n",
    "    xgb_model.fit(train_features, train_labels)\n",
    "\n",
    "    # 验证过程\n",
    "    val_features = []\n",
    "    val_labels = []\n",
    "\n",
    "    for images, structured_data, targets in val_loader:\n",
    "        images, structured_data = images.to(device), structured_data.to(device)\n",
    "        features = combined_model(images, structured_data)\n",
    "        val_features.append(features)\n",
    "        val_labels.append(targets.numpy())\n",
    "\n",
    "    val_features = np.vstack(val_features)\n",
    "    val_labels = np.hstack(val_labels)\n",
    "    \n",
    "    val_preds = xgb_model.predict_proba(val_features)[:, 1]\n",
    "    auc = roc_auc_score(val_labels, val_preds)\n",
    "    print(f\"Epoch {epoch+1}: AUC = {auc:.4f}\")\n",
    "\n",
    "    # 保存最佳模型\n",
    "    if auc > best_auc:\n",
    "        best_auc = auc\n",
    "        torch.save(combined_model.state_dict(), '0907best_combined_model.pth')\n",
    "        xgb_model.save_model('0907best_xgb_model.json')\n",
    "\n",
    "\n",
    "# 测试过程\n",
    "test_dataset = CustomDataset(\n",
    "    csv_file='data/test-metadata.csv',\n",
    "    img_hdf5_file='data/test-image.hdf5',\n",
    "    transform=transform,\n",
    "    is_test=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# 加载最佳模型\n",
    "combined_model.load_state_dict(torch.load('0907best_combined_model.pth'))\n",
    "xgb_model.load_model('0907best_xgb_model.json')\n",
    "\n",
    "test_image_names = []\n",
    "test_predictions = []\n",
    "\n",
    "for images, structured_data, image_ids in test_loader:\n",
    "    images, structured_data = images.to(device), structured_data.to(device)\n",
    "    features = combined_model(images, structured_data)\n",
    "    preds = xgb_model.predict_proba(features)[:, 1]\n",
    "    test_image_names.extend(image_ids)  # 这里不需要调用 numpy()\n",
    "    test_predictions.extend(preds)\n",
    "\n",
    "# 保存测试结果\n",
    "results = pd.DataFrame({'image_id': test_image_names, 'prediction': test_predictions})\n",
    "results.to_csv('test_predictions.csv', index=False)\n",
    "\n",
    "print(\"Testing completed. Predictions saved to test_predictions.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e926ff-fe2b-4c2c-991d-1e54ae3b80c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d49052-0185-4e16-884a-370b68e45762",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "735d9374-975b-4068-bd32-fb3cb1449189",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-06T17:52:47.785566Z",
     "iopub.status.busy": "2024-09-06T17:52:47.785140Z",
     "iopub.status.idle": "2024-09-07T00:16:10.395656Z",
     "shell.execute_reply": "2024-09-07T00:16:10.395074Z",
     "shell.execute_reply.started": "2024-09-06T17:52:47.785541Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: AUC = 0.8853\n",
      "Epoch 2: AUC = 0.9002\n",
      "Epoch 3: AUC = 0.8915\n",
      "Epoch 4: AUC = 0.8891\n",
      "Epoch 5: AUC = 0.9104\n",
      "Epoch 6: AUC = 0.8738\n",
      "Epoch 7: AUC = 0.9025\n",
      "Epoch 8: AUC = 0.8645\n",
      "Epoch 9: AUC = 0.8632\n",
      "Epoch 10: AUC = 0.9111\n",
      "Epoch 11: AUC = 0.8872\n",
      "Epoch 12: AUC = 0.9145\n",
      "Epoch 13: AUC = 0.9175\n",
      "Epoch 14: AUC = 0.8934\n",
      "Epoch 15: AUC = 0.8832\n",
      "Epoch 16: AUC = 0.8884\n",
      "Epoch 17: AUC = 0.8911\n",
      "Epoch 18: AUC = 0.8998\n",
      "Epoch 19: AUC = 0.9208\n",
      "Epoch 20: AUC = 0.8829\n",
      "Testing completed. Predictions saved to test_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "from PIL import Image\n",
    "import io\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "\n",
    "# 数据集类\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_hdf5_file, transform=None, is_test=False):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.img_hdf5_file = img_hdf5_file\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        \n",
    "        # 处理图像ID\n",
    "        self.image_ids = self.data['isic_id'].values\n",
    "        \n",
    "        # 处理结构化数据\n",
    "        if 'target' in self.data.columns:\n",
    "            self.structured_data = self.data.drop(columns=['isic_id', 'target'])\n",
    "        else:\n",
    "            self.structured_data = self.data.drop(columns=['isic_id'])\n",
    "        \n",
    "        # 特征工程：将字符串类型的列转换为数字\n",
    "        for col in self.structured_data.columns:\n",
    "            if self.structured_data[col].dtype == 'object':\n",
    "                self.structured_data[col] = self.structured_data[col].astype(str).str.extract('(\\d+)').astype(float)\n",
    "        \n",
    "        # 处理缺失值\n",
    "        self.structured_data = self.structured_data.fillna(0).values.astype(np.float32)\n",
    "        \n",
    "        # 特征标准化\n",
    "        self.scaler = StandardScaler()\n",
    "        self.structured_data = self.scaler.fit_transform(self.structured_data)\n",
    "        \n",
    "        if not is_test:\n",
    "            # 如果不是测试集，处理目标列\n",
    "            self.targets = self.data['target'].values.astype(np.float32)\n",
    "        else:\n",
    "            self.targets = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.img_hdf5_file, 'r') as hdf5_file:\n",
    "            image_key = self.image_ids[idx]\n",
    "            image_data = hdf5_file[image_key][()]\n",
    "            \n",
    "        image = Image.open(io.BytesIO(image_data))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        structured_data = torch.tensor(self.structured_data[idx], dtype=torch.float32)\n",
    "        if self.targets is not None:\n",
    "            target = torch.tensor(self.targets[idx], dtype=torch.float32)\n",
    "            return image, structured_data, target\n",
    "        else:\n",
    "            return image, structured_data, self.image_ids[idx]\n",
    "\n",
    "# 深度可分离卷积（Depthwise Separable Convolution）\n",
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, expand_ratio=1):\n",
    "        super(DepthwiseSeparableConv, self).__init__()\n",
    "        hidden_channels = in_channels * expand_ratio\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, hidden_channels, kernel_size=3, stride=stride, padding=1, groups=in_channels, bias=False),\n",
    "            nn.BatchNorm2d(hidden_channels),\n",
    "            nn.ReLU6(inplace=True),\n",
    "            nn.Conv2d(hidden_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "# MobileNetV2 的基本块（Block）\n",
    "class MobileNetV2Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride, expand_ratio):\n",
    "        super(MobileNetV2Block, self).__init__()\n",
    "        hidden_channels = in_channels * expand_ratio\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, hidden_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(hidden_channels),\n",
    "            nn.ReLU6(inplace=True),\n",
    "            DepthwiseSeparableConv(hidden_channels, out_channels, stride=stride, expand_ratio=expand_ratio)\n",
    "        )\n",
    "\n",
    "        self.use_residual = stride == 1 and in_channels == out_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_residual:\n",
    "            return x + self.block(x)\n",
    "        else:\n",
    "            return self.block(x)\n",
    "\n",
    "# MobileNetV2 模型\n",
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU6(inplace=True),\n",
    "            \n",
    "            MobileNetV2Block(32, 16, stride=1, expand_ratio=1),\n",
    "            MobileNetV2Block(16, 24, stride=2, expand_ratio=6),\n",
    "            MobileNetV2Block(24, 24, stride=1, expand_ratio=6),\n",
    "            MobileNetV2Block(24, 32, stride=2, expand_ratio=6),\n",
    "            MobileNetV2Block(32, 32, stride=1, expand_ratio=6),\n",
    "            MobileNetV2Block(32, 64, stride=2, expand_ratio=6),\n",
    "            MobileNetV2Block(64, 64, stride=1, expand_ratio=6),\n",
    "            MobileNetV2Block(64, 96, stride=1, expand_ratio=6),\n",
    "            MobileNetV2Block(96, 96, stride=1, expand_ratio=6),\n",
    "            MobileNetV2Block(96, 160, stride=2, expand_ratio=6),\n",
    "            MobileNetV2Block(160, 160, stride=1, expand_ratio=6),\n",
    "            MobileNetV2Block(160, 320, stride=1, expand_ratio=6),\n",
    "\n",
    "            nn.Conv2d(320, 1280, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(1280),\n",
    "            nn.ReLU6(inplace=True),\n",
    "\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(1280, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# 定义结合MobileNetV2特征和结构化特征的XGBoost模型\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, mobilenet_model, structured_data_dim):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.mobilenet = mobilenet_model\n",
    "        self.structured_data_dim = structured_data_dim\n",
    "\n",
    "    def forward(self, image, structured_data):\n",
    "        image_features = self.mobilenet(image).detach().cpu().numpy()\n",
    "        combined_features = np.hstack((image_features, structured_data.detach().cpu().numpy()))\n",
    "        return combined_features\n",
    "\n",
    "# 判断是否有可用的GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 数据变换\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((15, 15)),  # 修改为15x15图像尺寸\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 加载数据集\n",
    "train_dataset = CustomDataset(\n",
    "    csv_file='data/train_metadata_aligned.csv',\n",
    "    img_hdf5_file='data/train-image.hdf5',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# 划分训练集和验证集\n",
    "train_size = int(0.9 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_set, val_set = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=32, shuffle=False)\n",
    "\n",
    "# 初始化模型\n",
    "mobilenet_model = MobileNetV2(num_classes=1280).to(device)\n",
    "combined_model = CombinedModel(mobilenet_model, structured_data_dim=train_set[0][1].shape[0])\n",
    "\n",
    "# XGBoost分类器\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', n_estimators=100)\n",
    "\n",
    "# 训练与验证\n",
    "best_auc = 0\n",
    "for epoch in range(20):  # 这里只用1个epoch演示，实际应使用更多\n",
    "    train_features = []\n",
    "    train_labels = []\n",
    "\n",
    "    # 训练过程\n",
    "    for images, structured_data, targets in train_loader:\n",
    "        images, structured_data = images.to(device), structured_data.to(device)\n",
    "        features = combined_model(images, structured_data)\n",
    "        train_features.append(features)\n",
    "        train_labels.append(targets.numpy())\n",
    "    \n",
    "    train_features = np.vstack(train_features)\n",
    "    train_labels = np.hstack(train_labels)\n",
    "\n",
    "    xgb_model.fit(train_features, train_labels)\n",
    "\n",
    "    # 验证过程\n",
    "    val_features = []\n",
    "    val_labels = []\n",
    "\n",
    "    for images, structured_data, targets in val_loader:\n",
    "        images, structured_data = images.to(device), structured_data.to(device)\n",
    "        features = combined_model(images, structured_data)\n",
    "        val_features.append(features)\n",
    "        val_labels.append(targets.numpy())\n",
    "\n",
    "    val_features = np.vstack(val_features)\n",
    "    val_labels = np.hstack(val_labels)\n",
    "    \n",
    "    val_preds = xgb_model.predict_proba(val_features)[:, 1]\n",
    "    auc = roc_auc_score(val_labels, val_preds)\n",
    "    print(f\"Epoch {epoch+1}: AUC = {auc:.4f}\")\n",
    "\n",
    "    # 保存最佳模型\n",
    "    if auc > best_auc:\n",
    "        best_auc = auc\n",
    "        torch.save(combined_model.state_dict(), '0906best_combined_model.pth')\n",
    "        xgb_model.save_model('0906best_xgb_model.json')\n",
    "\n",
    "\n",
    "# 测试过程\n",
    "test_dataset = CustomDataset(\n",
    "    csv_file='data/test-metadata.csv',\n",
    "    img_hdf5_file='data/test-image.hdf5',\n",
    "    transform=transform,\n",
    "    is_test=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# 加载最佳模型\n",
    "combined_model.load_state_dict(torch.load('0906best_combined_model.pth'))\n",
    "xgb_model.load_model('0906best_xgb_model.json')\n",
    "\n",
    "test_image_names = []\n",
    "test_predictions = []\n",
    "\n",
    "for images, structured_data, image_ids in test_loader:\n",
    "    images, structured_data = images.to(device), structured_data.to(device)\n",
    "    features = combined_model(images, structured_data)\n",
    "    preds = xgb_model.predict_proba(features)[:, 1]\n",
    "    test_image_names.extend(image_ids)  # 这里不需要调用 numpy()\n",
    "    test_predictions.extend(preds)\n",
    "\n",
    "# 保存测试结果\n",
    "results = pd.DataFrame({'image_id': test_image_names, 'prediction': test_predictions})\n",
    "results.to_csv('test_predictions.csv', index=False)\n",
    "\n",
    "print(\"Testing completed. Predictions saved to test_predictions.csv\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
